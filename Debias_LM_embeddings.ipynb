{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    topic = 'gender'\n",
    "    vocabPath = f\"data/{topic}_attributes_optm.json\"\n",
    "    male_words = \"data/male_word_file.txt\"\n",
    "    female_words = \"data/female_word_file.txt\"\n",
    "    words = \"data/reddit.US.txt.tok.clean.cleanedforw2v_1.w2v\"\n",
    "    male_embedddings = f'{model_name.replace(\"/\", \"-\")}-male-embeddings.npy'\n",
    "    female_embedddings = f'{model_name.replace(\"/\", \"-\")}-female-embeddings.npy'\n",
    "    word_embedddings = f'word_embeddings/{model_name.replace(\"/\", \"-\")}-embeddings.npy'\n",
    "    outprefix = model_name.replace(\"/\", \"-\")+\"-\"+vocabPath.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\".\", \"_\")\n",
    "    stereoset_file = \"data/dev.json\"\n",
    "    dimensions = 1024\n",
    "    mode = \"role\"\n",
    "    subspace_dim = 14\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aishik/anaconda3/envs/R106/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.stats import ttest_rel, spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "import wefe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidWord(word):\n",
    "    return all([c.isalpha() for c in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruneWordVecs(wordVecs):\n",
    "    newWordVecs = {}\n",
    "    for word, vec in wordVecs.items():\n",
    "        valid=True\n",
    "        if(not isValidWord(word)):\n",
    "            valid = False\n",
    "        if(valid):\n",
    "            newWordVecs[word] = vec\n",
    "    return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words(w2v_file):\n",
    "    words = []\n",
    "    with open(w2v_file, 'r') as f:\n",
    "        for line in f:\n",
    "            vect = line.strip().rsplit()\n",
    "            word = vect[0]\n",
    "            words.append(word)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = load_words(CFG.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(CFG.model_name).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analogy_templates(json_filepath, mode):\n",
    "\twith open(json_filepath, \"r\") as f:\t\n",
    "\t\tloadedData = json.load(f)\n",
    "\t\treturn loadedData[\"analogy_templates\"][mode]\n",
    "\n",
    "def load_test_terms(json_filepath):\n",
    "\twith open(json_filepath, \"r\") as f:\t\n",
    "\t\tloadedData = json.load(f)\n",
    "\t\treturn loadedData[\"testTerms\"]\n",
    "\n",
    "def load_eval_terms(json_filepath, mode):\n",
    "\twith open(json_filepath, \"r\") as f:\t\n",
    "\t\tloadedData = json.load(f)\n",
    "\t\treturn loadedData[\"eval_targets\"], loadedData[\"analogy_templates\"][mode].values()\n",
    "\n",
    "def load_def_sets(json_filepath):\n",
    "\twith open(json_filepath, \"r\") as f: \n",
    "\t\tloadedData = json.load(f)\n",
    "\t\treturn {i: v for i, v in enumerate(loadedData[\"definite_sets\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Vocabulary\")\n",
    "analogyTemplates = load_analogy_templates(CFG.vocabPath, CFG.mode)\n",
    "defSets = load_def_sets(CFG.vocabPath)\n",
    "testTerms = load_test_terms(CFG.vocabPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'man': ['manager', 'executive', 'doctor', 'lawyer', 'programmer', 'scientist', 'soldier', 'supervisor', 'rancher', 'janitor', 'firefighter', 'officer'], 'woman': ['secretary', 'nurse', 'clerk', 'artist', 'homemaker', 'dancer', 'singer', 'librarian', 'maid', 'hairdresser', 'stylist', 'receptionist', 'counselor']}\n"
     ]
    }
   ],
   "source": [
    "print(analogyTemplates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['he', 'she'], 1: ['his', 'hers'], 2: ['son', 'daughter'], 3: ['father', 'mother'], 4: ['male', 'female'], 5: ['boy', 'girl'], 6: ['uncle', 'aunt']}\n"
     ]
    }
   ],
   "source": [
    "print(defSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral Words ['manager', 'executive', 'doctor', 'lawyer', 'programmer', 'scientist', 'soldier', 'supervisor', 'rancher', 'janitor', 'firefighter', 'officer', 'secretary', 'nurse', 'clerk', 'artist', 'homemaker', 'dancer', 'singer', 'librarian', 'maid', 'hairdresser', 'stylist', 'receptionist', 'counselor']\n"
     ]
    }
   ],
   "source": [
    "neutral_words = []\n",
    "for value in analogyTemplates.values():\n",
    "    neutral_words.extend(value)\n",
    "print(f\"Neutral Words {neutral_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_word_embeddings = model.encode(neutral_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "neutral_embedding_dict = {word: embedding for word, embedding in zip(neutral_words, neutral_word_embeddings)}\n",
    "embedding_dim = neutral_word_embeddings.shape[-1]\n",
    "CFG.embedding_dim = embedding_dim\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bias_subspace(vocab, def_sets, subspace_dim, embedding_dim):\n",
    "    \"\"\"\n",
    "    Similar to bolukbasi's implementation at\n",
    "    https://github.com/tolga-b/debiaswe/blob/master/debiaswe/debias.py\n",
    "\n",
    "    vocab - dictionary mapping words to embeddings\n",
    "    def_sets - sets of words that represent extremes? of the subspace\n",
    "            we're interested in (e.g. man-woman, boy-girl, etc. for binary gender)\n",
    "    subspace_dim - number of vectors defining the subspace\n",
    "    embedding_dim - dimensions of the word embeddings\n",
    "    \"\"\"\n",
    "    # calculate means of defining sets\n",
    "    means = {}\n",
    "    for k, v in def_sets.items():\n",
    "        wSet = []\n",
    "        for w in v:\n",
    "            try:\n",
    "                wSet.append(vocab[w])\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        set_vectors = np.array(wSet)\n",
    "        means[k] = np.mean(set_vectors, axis=0)\n",
    "\n",
    "    # calculate vectors to perform PCA\n",
    "    matrix = []\n",
    "    for k, v in def_sets.items():\n",
    "        wSet = []\n",
    "        for w in v:\n",
    "            try:\n",
    "                wSet.append(vocab[w])\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        set_vectors = np.array(wSet)\n",
    "        diffs = set_vectors - means[k]\n",
    "        matrix.append(diffs)\n",
    "\n",
    "    matrix = np.concatenate(matrix)\n",
    "\n",
    "    pca = PCA(n_components=subspace_dim)\n",
    "    pca.fit(matrix)\n",
    "\n",
    "    return pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided Python function `identify_bias_subspace` is designed to identify a subspace in a word embedding space that captures a certain bias. This is based on the method proposed by Bolukbasi et al. in their paper \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\". The function takes four arguments: `vocab`, `def_sets`, `subspace_dim`, and `embedding_dim`.\n",
    "\n",
    "The `vocab` argument is a dictionary where the keys are words and the values are their corresponding embeddings. `def_sets` is a dictionary where each key-value pair represents a set of words that define an extreme of the subspace of interest. For example, in the case of binary gender, you might have sets like {man, boy} and {woman, girl}. `subspace_dim` is the number of vectors that define the subspace, and `embedding_dim` is the dimensionality of the word embeddings.\n",
    "\n",
    "The function first calculates the mean vector of each defining set. It does this by iterating over the words in each set, retrieving their embeddings from the `vocab` dictionary, and then computing the mean of these vectors.\n",
    "\n",
    "Next, the function calculates a matrix where each row is a vector representing the difference between a word's embedding and the mean vector of its defining set. This is done by iterating over the words in each defining set again, retrieving their embeddings, and subtracting the mean vector of the set from each embedding.\n",
    "\n",
    "This matrix is then used to perform Principal Component Analysis (PCA) with the number of components equal to `subspace_dim`. PCA is a technique used to reduce the dimensionality of data while preserving as much of the data's original variance as possible. The PCA components that are returned by the function represent the directions in the embedding space that capture the most variance in the data. These directions define the bias subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CFG.male_words, 'r') as file:\n",
    "    # Read the file\n",
    "    data = file.read()\n",
    "\n",
    "# Split the file into words\n",
    "male_words = data.split()\n",
    "with open(CFG.female_words, 'r') as file:\n",
    "    # Read the file\n",
    "    data = file.read()\n",
    "\n",
    "# Split the file into words\n",
    "female_words = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = np.load(CFG.word_embedddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {word: embedding for word, embedding in zip(words, word_embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = pruneWordVecs(embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['he', 'she'], 1: ['his', 'hers'], 2: ['son', 'daughter'], 3: ['father', 'mother'], 4: ['male', 'female'], 5: ['boy', 'girl'], 6: ['uncle', 'aunt']}\n"
     ]
    }
   ],
   "source": [
    "print(defSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 1024)\n"
     ]
    }
   ],
   "source": [
    "subspace = identify_bias_subspace(embedding_dict, defSets, CFG.subspace_dim, embedding_dim)[:CFG.subspace_dim]\n",
    "print(subspace.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_subspace(vector, subspace):\n",
    "    v_b = np.zeros_like(vector)\n",
    "    for component in subspace:\n",
    "        v_b += np.dot(vector.transpose(), component) * component\n",
    "    return v_b\n",
    "\n",
    "def normalize(word_vectors):\n",
    "    for k, v in word_vectors.items():\n",
    "        word_vectors[k] = v / np.linalg.norm(v)\n",
    "\n",
    "def neutralize_and_equalize(vocab, words, eq_sets, bias_subspace, embedding_dim):\n",
    "    \"\"\"\n",
    "    vocab - dictionary mapping words to embeddings\n",
    "    words - words to neutralize\n",
    "    eq_sets - set of equality sets\n",
    "    bias_subspace - subspace of bias from identify_bias_subspace\n",
    "    embedding_dim - dimensions of the word embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    if bias_subspace.ndim == 1:\n",
    "        bias_subspace = np.expand_dims(bias_subspace, 0)\n",
    "    elif bias_subspace.ndim != 2:\n",
    "        raise ValueError(\"bias subspace should be either a matrix or vector\")\n",
    "\n",
    "    new_vocab = vocab.copy()\n",
    "    for w in words:\n",
    "        # get projection onto bias subspace\n",
    "        if w in vocab:\n",
    "            v = vocab[w]\n",
    "            v_b = project_onto_subspace(v, bias_subspace)\n",
    "\n",
    "            new_v = (v - v_b) / np.linalg.norm(v - v_b)\n",
    "            #print np.linalg.norm(new_v)\n",
    "            # update embedding\n",
    "            new_vocab[w] = new_v\n",
    "\n",
    "    normalize(new_vocab)\n",
    "\n",
    "    for eq_set in eq_sets:\n",
    "        mean = np.zeros((embedding_dim,))\n",
    "\n",
    "        #Make sure the elements in the eq sets are valid\n",
    "        cleanEqSet = []\n",
    "        for w in eq_set:\n",
    "            try:\n",
    "                _ = new_vocab[w]\n",
    "                cleanEqSet.append(w)\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "        for w in cleanEqSet:\n",
    "            mean += new_vocab[w]\n",
    "        mean /= float(len(cleanEqSet))\n",
    "\n",
    "        mean_b = project_onto_subspace(mean, bias_subspace)\n",
    "        upsilon = mean - mean_b\n",
    "\n",
    "        for w in cleanEqSet:\n",
    "            v = new_vocab[w]\n",
    "            v_b = project_onto_subspace(v, bias_subspace)\n",
    "\n",
    "            frac = (v_b - mean_b) / np.linalg.norm(v_b - mean_b)\n",
    "            new_v = upsilon + np.sqrt(1 - np.sum(np.square(upsilon))) * frac\n",
    "\n",
    "            new_vocab[w] = new_v\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hard_word_vectors = neutralize_and_equalize(embedding_dict, neutral_words, defSets.values(), subspace, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_432538/1368562009.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  Neutrals = torch.tensor([vocab[w] for w in words]).float().t()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss @ Epoch #0: tensor(27244242., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #1: tensor(24607844., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #2: tensor(22229754., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #3: tensor(20084642., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #4: tensor(18150220., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #5: tensor(16406372., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #6: tensor(14834380., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #7: tensor(13417948., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #8: tensor(12142181., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #9: tensor(10993980., grad_fn=<AddBackward0>)\n",
      "Optimization Completed, normalizing vector transform\n"
     ]
    }
   ],
   "source": [
    "def equalize_and_soften(vocab, words, eq_sets, bias_subspace, embedding_dim, l=0.2, verbose=True):\n",
    "    vocabIndex, vocabVectors = zip(*vocab.items())\n",
    "    vocabIndex = {i:label for i, label in enumerate(vocabIndex)}\n",
    "    Neutrals = torch.tensor([vocab[w] for w in words]).float().t()\n",
    "\n",
    "    Words = torch.tensor(vocabVectors).float().t()\n",
    "\n",
    "    # perform SVD on W to reduce memory and computational costs\n",
    "    # based on suggestions in supplementary material of Bolukbasi et al.\n",
    "    u, s, _ = torch.svd(Words)\n",
    "    s = torch.diag(s)\n",
    "\n",
    "    # precompute\n",
    "    t1 = s.mm(u.t())\n",
    "    t2 = u.mm(s)\n",
    "\n",
    "    Transform = torch.randn(embedding_dim, embedding_dim).float()\n",
    "    BiasSpace = torch.tensor(bias_subspace).reshape(embedding_dim, -1).float()\n",
    "\n",
    "    Neutrals.requires_grad = False\n",
    "    Words.requires_grad = False\n",
    "    BiasSpace.requires_grad = False\n",
    "    Transform.requires_grad = True\n",
    "\n",
    "    epochs = 10\n",
    "    optimizer = torch.optim.SGD([Transform], lr=0.000001, momentum=0.0)\n",
    "\n",
    "    for i in range(0, epochs):\n",
    "        TtT = torch.mm(Transform.t(), Transform)\n",
    "        norm1 = (t1.mm(TtT - torch.eye(embedding_dim)).mm(t2)).norm(p=2)\n",
    "\n",
    "        norm2 = (Neutrals.t().mm(TtT).mm(BiasSpace)).norm(p=2)\n",
    "\n",
    "        loss = norm1 + l * norm2\n",
    "        norm1 = None\n",
    "        norm2 = None\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Loss @ Epoch #\" + str(i) + \":\", loss)\n",
    "\n",
    "    if(verbose):\n",
    "        print(\"Optimization Completed, normalizing vector transform\")\n",
    "\n",
    "    debiasedVectors = {}\n",
    "    for i, w in enumerate(Words.t()):\n",
    "        transformedVec = torch.mm(Transform, w.view(-1, 1))\n",
    "        debiasedVectors[vocabIndex[i]] = ( transformedVec / transformedVec.norm(p=2) ).detach().numpy().flatten()\n",
    "\n",
    "    return debiasedVectors\n",
    "\n",
    "new_soft_word_vectors = equalize_and_soften(embedding_dict, neutral_words, defSets.values(), subspace, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `equalize_and_soften` function in Python is designed to debias word embeddings. It takes a vocabulary of words with their vector representations, a list of neutral words, a bias subspace, and the dimension of the embeddings as inputs.\n",
    "\n",
    "The function first prepares the data and performs Singular Value Decomposition (SVD) on the word embeddings to reduce computational costs. It then initializes a transformation matrix and a tensor representing the bias subspace.\n",
    "\n",
    "The function uses Stochastic Gradient Descent (SGD) to optimize the transformation matrix over several epochs. The loss function it minimizes consists of two parts: one measures the difference between the dot product of the transformed word embeddings and the identity matrix, and the other measures the projection of the transformed neutral words onto the bias subspace.\n",
    "\n",
    "After the optimization, the function applies the learned transformation to each word in the vocabulary, normalizes the transformed vectors, and returns a dictionary mapping each word to its debiased vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def equalize_and_soften(vocab, words, eq_sets, bias_subspace, embedding_dim, l=0.2, verbose=True):\n",
    "#     vocabIndex, vocabVectors = zip(*vocab.items())\n",
    "#     vocabIndex = {i:label for i, label in enumerate(vocabIndex)}\n",
    "#     Neutrals = torch.tensor([vocab[w] for w in words]).float().t()\n",
    "\n",
    "#     Words = torch.tensor(vocabVectors).float().t()\n",
    "\n",
    "#     # perform SVD on W to reduce memory and computational costs\n",
    "#     # based on suggestions in supplementary material of Bolukbasi et al.\n",
    "#     u, s, _ = torch.svd(Words)\n",
    "#     s = torch.diag(s)\n",
    "\n",
    "#     # precompute\n",
    "#     t1 = s.mm(u.t())\n",
    "#     t2 = u.mm(s)\n",
    "\n",
    "#     Transform = torch.randn(embedding_dim, embedding_dim).float()\n",
    "#     BiasSpace = torch.tensor(bias_subspace).reshape(embedding_dim, -1).float()\n",
    "\n",
    "#     Neutrals.requires_grad = False\n",
    "#     Words.requires_grad = False\n",
    "#     BiasSpace.requires_grad = False\n",
    "#     Transform.requires_grad = True\n",
    "\n",
    "#     epochs = 50\n",
    "#     optimizer = torch.optim.Adam([Transform], lr=0.001)\n",
    "\n",
    "#     for i in range(0, epochs):\n",
    "#         TtT = torch.mm(Transform.t(), Transform)\n",
    "#         norm1 = (t1.mm(TtT - torch.eye(embedding_dim)).mm(t2)).norm(p=2)\n",
    "\n",
    "#         norm2 = (Neutrals.t().mm(TtT).mm(BiasSpace)).norm(p=2)\n",
    "\n",
    "#         loss = norm1 + l * norm2\n",
    "#         norm1 = None\n",
    "#         norm2 = None\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         if(verbose):\n",
    "#             print(\"Loss @ Epoch #\" + str(i) + \":\", loss)\n",
    "\n",
    "#     if(verbose):\n",
    "#         print(\"Optimization Completed, normalizing vector transform\")\n",
    "\n",
    "#     debiasedVectors = {}\n",
    "#     for i, w in enumerate(Words.t()):\n",
    "#         transformedVec = torch.mm(Transform, w.view(-1, 1))\n",
    "#         debiasedVectors[vocabIndex[i]] = ( transformedVec / transformedVec.norm(p=2) ).detach().numpy().flatten()\n",
    "\n",
    "#     return debiasedVectors\n",
    "\n",
    "# new_soft_word_vectors = equalize_and_soften(embedding_dict, neutral_words, defSets.values(), subspace, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss @ Epoch #0: tensor(23075.6855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #1: tensor(7093.2671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #2: tensor(4252.2319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #3: tensor(6723.2251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #4: tensor(7975.2109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #5: tensor(6362.8672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #6: tensor(4111.3345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #7: tensor(3010.6296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #8: tensor(3059.9114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #9: tensor(3188.7981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #10: tensor(2858.2917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #11: tensor(2366.5796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #12: tensor(2112.9055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #13: tensor(2076.6985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #14: tensor(1975.0974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #15: tensor(1703.4856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #16: tensor(1424.8606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #17: tensor(1297.5143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #18: tensor(1284.1241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #19: tensor(1246.5001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #20: tensor(1127.8771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #21: tensor(990.3204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #22: tensor(902.7027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #23: tensor(861.4829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #24: tensor(824.6786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #25: tensor(774.2008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #26: tensor(724.4925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #27: tensor(686.8284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #28: tensor(653.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #29: tensor(613.7662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #30: tensor(575.4406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #31: tensor(549.6833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #32: tensor(536.8777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #33: tensor(525.8933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #34: tensor(506.3952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #35: tensor(480.3632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #36: tensor(457.6681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #37: tensor(443.4921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #38: tensor(435.4119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #39: tensor(427.8242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #40: tensor(417.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #41: tensor(404.2079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #42: tensor(392.4883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #43: tensor(383.6057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #44: tensor(376.9656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #45: tensor(370.9834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #46: tensor(364.4474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #47: tensor(357.2850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #48: tensor(350.2425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #49: tensor(343.9803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Optimization Completed, normalizing vector transform\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "class TransformNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TransformNet, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def equalize_and_soften_new(vocab, words, eq_sets, bias_subspace, embedding_dim, l=0.2, verbose=True):\n",
    "    vocabIndex, vocabVectors = zip(*vocab.items())\n",
    "    vocabIndex = {i:label for i, label in enumerate(vocabIndex)}\n",
    "    Neutrals = torch.tensor([vocab[w] for w in words]).float().t().to(\"cuda\")\n",
    "\n",
    "    Words = torch.tensor(vocabVectors).float().t().to(\"cuda\")\n",
    "\n",
    "    Transform = TransformNet(embedding_dim, embedding_dim).to(\"cuda\")\n",
    "    BiasSpace = torch.tensor(bias_subspace).reshape(embedding_dim, -1).float().to(\"cuda\")\n",
    "\n",
    "    Neutrals.requires_grad = False\n",
    "    Words.requires_grad = False\n",
    "    BiasSpace.requires_grad = False\n",
    "\n",
    "    epochs = 50\n",
    "    optimizer = optim.Adam(Transform.parameters(), lr=0.001)\n",
    "\n",
    "    identity_matrix = torch.eye(embedding_dim).to(\"cuda\")\n",
    "\n",
    "    Words = Words.t()\n",
    "    for i in range(0, epochs):\n",
    "        transformed_words = Transform(Words)\n",
    "        norm1 = torch.norm(torch.matmul(transformed_words.t(), transformed_words) - identity_matrix)\n",
    "\n",
    "        norm2 = torch.norm(torch.matmul(Neutrals.t(), transformed_words.t()))\n",
    "\n",
    "        loss = norm1 + l * norm2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Loss @ Epoch #\" + str(i) + \":\", loss)\n",
    "\n",
    "    if(verbose):\n",
    "        print(\"Optimization Completed, normalizing vector transform\")\n",
    "\n",
    "    debiasedVectors = {}\n",
    "    for i, w in enumerate(Words):\n",
    "        transformedVec = Transform(w.view(1, -1))\n",
    "        debiasedVectors[vocabIndex[i]] = ( transformedVec / transformedVec.norm(p=2) ).detach().cpu().numpy().flatten()\n",
    "\n",
    "    return debiasedVectors\n",
    "\n",
    "new_new_soft_word_vectors = equalize_and_soften_new(embedding_dict, neutral_words, defSets.values(), subspace, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python code defines a class `TransformNet` that inherits from PyTorch's `nn.Module`. It represents a simple neural network with a single linear transformation layer.\n",
    "\n",
    "The `equalize_and_soften` function debiases word embeddings. It prepares the data, initializes a transformation network and a bias tensor, and optimizes the transformation network using Adam optimizer over several epochs. The loss function it minimizes consists of two parts: one measures the difference between the dot product of the transformed word embeddings and the identity matrix, and the other measures the projection of the transformed neutral words onto the bias subspace.\n",
    "\n",
    "After the optimization, the function applies the learned transformation to each word in the vocabulary, normalizes the transformed vectors, and returns a dictionary mapping each word to its debiased vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebiasedSentenceTransformer():\n",
    "    def __init__(self, model, debiasModel):\n",
    "        self.model = model\n",
    "        self.debias_model = debiasModel\n",
    "\n",
    "    def encode(self, word):\n",
    "        sentence_embedding = self.model.encode(word)\n",
    "        sentence_embedding = np.array(sentence_embedding)\n",
    "        transformedVec = self.debias_model(word.view(1, -1))\n",
    "        debiased_vec = ( transformedVec / transformedVec.norm(p=2) ).detach().cpu().numpy().flatten()\n",
    "        return debiased_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoredAnalogyAnswers(a,b,x, keyedVecs, thresh=12.5):\n",
    "\twords = [w for w in keyedVecs.key_to_index.keys() if np.linalg.norm(np.array(keyedVecs[w])-np.array(keyedVecs[x])) < thresh]\n",
    "\n",
    "\tdef cos(a,b,x,y):\n",
    "\t\taVec = np.array(keyedVecs[a])\n",
    "\t\tbVec = np.array(keyedVecs[b])\n",
    "\t\txVec = np.array(keyedVecs[x])\n",
    "\t\tyVec = np.array(keyedVecs[y])\n",
    "\t\tnumerator = (aVec-bVec).dot(xVec-yVec)\n",
    "\t\tdenominator = np.linalg.norm(aVec-bVec)*np.linalg.norm(xVec-yVec)\n",
    "\t\treturn numerator/(denominator if denominator != 0 else 1e-6)\n",
    "\n",
    "\treturn sorted([(cos(a,b,x,y), a,b,x,y) for y in words], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAnalogies(analogyTemplates, keyedVecs):\n",
    "\texpandedAnalogyTemplates = []\n",
    "\tfor A, stereotypes in analogyTemplates.items():\n",
    "\t\tfor B, _ in analogyTemplates.items():\n",
    "\t\t\tif(A != B):\n",
    "\t\t\t\tfor stereotype in stereotypes:\n",
    "\t\t\t\t\texpandedAnalogyTemplates.append([A, stereotype, B])\n",
    "\n",
    "\tanalogies = []\n",
    "\toutputGroups = []\n",
    "\tfor a,b,x in expandedAnalogyTemplates:\n",
    "\t\toutputs = scoredAnalogyAnswers(a,b,x,keyedVecs)\n",
    "\t\tformattedOutput = []\n",
    "\t\t\n",
    "\t\tfor score, a_w, b_w, x_w, y_w in outputs:\n",
    "\t\t\t\n",
    "\t\t\tanalogy = str(a_w) + \" is to \" + str(b_w) + \" as \" + str(x_w) + \" is to \" + str(y_w)\n",
    "\t\t\tanalogyRaw = [a_w, b_w, x_w, y_w]\n",
    "\t\t\tanalogies.append([score, analogy, analogyRaw])\n",
    "\t\t\tformattedOutput.append([score, analogy, analogyRaw])\n",
    "\t\toutputGroups.append(formattedOutput)\n",
    "\n",
    "\tanalogies = sorted(analogies, key=lambda x:-x[0])\n",
    "\treturn analogies, outputGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_legacy_to_keyvec(legacy_w2v):\n",
    "    dim = len(legacy_w2v[list(legacy_w2v.keys())[0]])\n",
    "    vectors = Word2VecKeyedVectors(dim)\n",
    "\n",
    "    ws = []\n",
    "    vs = []\n",
    "\n",
    "    for word, vect in legacy_w2v.items():\n",
    "        ws.append(word)\n",
    "        vs.append(vect)\n",
    "        assert(len(vect) == dim)\n",
    "    vectors.add_vectors(ws, vs, replace=True)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'man': ['manager', 'executive', 'doctor', 'lawyer', 'programmer', 'scientist', 'soldier', 'supervisor', 'rancher', 'janitor', 'firefighter', 'officer'], 'woman': ['secretary', 'nurse', 'clerk', 'artist', 'homemaker', 'dancer', 'singer', 'librarian', 'maid', 'hairdresser', 'stylist', 'receptionist', 'counselor']}\n"
     ]
    }
   ],
   "source": [
    "print(analogyTemplates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasedAnalogies, biasedAnalogyGroups = generateAnalogies(analogyTemplates, convert_legacy_to_keyvec(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardDebiasedAnalogies, hardDebiasedAnalogyGroups = generateAnalogies(analogyTemplates, convert_legacy_to_keyvec(new_hard_word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "softDebiasedAnalogies, softDebiasedAnalogyGroups = generateAnalogies(analogyTemplates, convert_legacy_to_keyvec(new_soft_word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_softDebiasedAnalogies, new_softDebiasedAnalogyGroups = generateAnalogies(analogyTemplates, convert_legacy_to_keyvec(new_new_soft_word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeAnalogies(analogies, path):\n",
    "    f = open(path, \"w\")\n",
    "    f.write(\"Score,Analogy\\n\")\n",
    "    for score, analogy, raw in analogies:\n",
    "        f.write(str(score) + \",\" + str(analogy) + \",\" + str(raw) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "def writeGroupAnalogies(groups, path):\n",
    "    f = open(path, \"w\")\n",
    "    f.write(\"Score,Analogy\\n\")\n",
    "    for analogies in groups:\n",
    "        for score, analogy, raw in analogies:\n",
    "            f.write(str(score) + \",\" + str(analogy) + \",\" + str(raw) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeAnalogies(biasedAnalogies, \"output/\" + CFG.outprefix + \"_biasedAnalogiesOut.csv\")\n",
    "writeGroupAnalogies(biasedAnalogyGroups, \"output/\" + CFG.outprefix + \"_biasedAnalogiesOut_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeAnalogies(hardDebiasedAnalogies, \"output/\" + CFG.outprefix + \"_hardDebiasedAnalogiesOut.csv\")\n",
    "writeGroupAnalogies(hardDebiasedAnalogyGroups, \"output/\" + CFG.outprefix + \"_hardDebiasedAnalogiesOut_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeAnalogies(softDebiasedAnalogies, \"output/\" + CFG.outprefix + \"_softDebiasedAnalogiesOut.csv\")\n",
    "writeGroupAnalogies(softDebiasedAnalogyGroups, \"output/\" + CFG.outprefix + \"_softDebiasedAnalogiesOut_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeAnalogies(new_softDebiasedAnalogies, \"output/\" + CFG.outprefix + \"_new_softDebiasedAnalogiesOut.csv\")\n",
    "writeGroupAnalogies(new_softDebiasedAnalogyGroups, \"output/\" + CFG.outprefix + \"_new_softDebiasedAnalogiesOut_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58663553 woman is to maid as man is to maid\n",
      "0.57359684 woman is to artist as man is to artist\n",
      "0.5720325 woman is to secretary as man is to secretary\n",
      "0.56268376 man is to firefighter as woman is to firefighter\n",
      "0.54958075 man is to rancher as woman is to rancher\n",
      "0.54686826 woman is to hairdresser as man is to hairdresser\n",
      "0.5389029 man is to supervisor as woman is to supervisor\n",
      "0.5362094 woman is to receptionist as man is to wobbles\n",
      "0.52826476 woman is to homemaker as man is to homemaker\n",
      "0.52508116 woman is to receptionist as man is to receptionist\n"
     ]
    }
   ],
   "source": [
    "for score, analogy, _ in biasedAnalogies[:10]:\n",
    "    print(score, analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00929687691981717"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([score for (score,_,_) in biasedAnalogies])\n",
    "np.mean([score for (score,_,_) in biasedAnalogies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59924316 woman is to maid as man is to maid\n",
      "0.5832432 woman is to artist as man is to artist\n",
      "0.5671364 woman is to secretary as man is to secretary\n",
      "0.55536425 man is to rancher as woman is to rancher\n",
      "0.55404055 man is to firefighter as woman is to firefighter\n",
      "0.55226827 woman is to hairdresser as man is to hairdresser\n",
      "0.54285866 man is to supervisor as woman is to supervisor\n",
      "0.5362576 woman is to receptionist as man is to wobbles\n",
      "0.53297687 woman is to homemaker as man is to homemaker\n",
      "0.5320646 woman is to receptionist as man is to receptionist\n",
      "0.5313066 man is to executive as woman is to executive\n",
      "0.5263474 man is to janitor as woman is to janitor\n",
      "0.5255713 man is to manager as woman is to manager\n",
      "0.5153393 woman is to singer as man is to singer\n",
      "0.5090588 man is to doctor as woman is to doctor\n",
      "0.50003546 woman is to counselor as man is to counselor\n",
      "0.49472508 man is to programmer as woman is to programmer\n",
      "0.4901575 man is to lawyer as woman is to lawyer\n",
      "0.48815748 woman is to stylist as man is to stylist\n",
      "0.48513496 woman is to librarian as man is to librarian\n"
     ]
    }
   ],
   "source": [
    "for score, analogy, _ in hardDebiasedAnalogies[:20]:\n",
    "    print(score, analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5804332 woman is to artist as man is to artist\n",
      "0.57672876 man is to janitor as woman is to janitor\n",
      "0.57596534 woman is to maid as man is to maid\n",
      "0.5732108 man is to firefighter as woman is to firefighter\n",
      "0.5706194 woman is to hairdresser as man is to hairdresser\n",
      "0.5687649 woman is to secretary as man is to secretary\n",
      "0.5537481 man is to doctor as woman is to doctor\n",
      "0.55258983 woman is to receptionist as man is to wobbles\n",
      "0.549801 woman is to singer as man is to singer\n",
      "0.54964745 man is to rancher as woman is to rancher\n",
      "0.54305786 man is to supervisor as woman is to supervisor\n",
      "0.54018116 man is to executive as woman is to executive\n",
      "0.53811675 woman is to receptionist as man is to receptionist\n",
      "0.5318387 man is to manager as woman is to manager\n",
      "0.52229124 woman is to homemaker as man is to homemaker\n",
      "0.5210902 woman is to stylist as man is to stylist\n",
      "0.5174539 man is to lawyer as woman is to lawyer\n",
      "0.5028639 woman is to librarian as man is to librarian\n",
      "0.50213295 woman is to receptionist as man is to instrumentation\n",
      "0.5015663 woman is to counselor as man is to counselor\n"
     ]
    }
   ],
   "source": [
    "for score, analogy, _ in softDebiasedAnalogies[:20]:\n",
    "    print(score, analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49238294 man is to doctor as woman is to doctor\n",
      "0.49028364 woman is to artist as man is to artist\n",
      "0.48353973 man is to firefighter as woman is to firefighter\n",
      "0.47399014 woman is to maid as man is to maid\n",
      "0.47046086 man is to executive as woman is to executive\n",
      "0.46987635 woman is to hairdresser as man is to hairdresser\n",
      "0.46800184 man is to supervisor as woman is to supervisor\n",
      "0.4650674 man is to janitor as woman is to janitor\n",
      "0.46499842 woman is to receptionist as man is to wobbles\n",
      "0.46427107 woman is to receptionist as man is to receptionist\n",
      "0.46295837 woman is to dancer as man is to dancer\n",
      "0.46180463 man is to manager as woman is to manager\n",
      "0.46035358 man is to programmer as woman is to programmer\n",
      "0.45054716 woman is to homemaker as man is to homemaker\n",
      "0.44868422 woman is to secretary as man is to secretary\n",
      "0.4452356 man is to lawyer as woman is to lawyer\n",
      "0.44465998 woman is to librarian as man is to librarian\n",
      "0.44363627 man is to rancher as woman is to rancher\n",
      "0.44246003 woman is to stylist as man is to stylist\n",
      "0.43983075 woman is to counselor as man is to counselor\n"
     ]
    }
   ],
   "source": [
    "for score, analogy, _ in new_softDebiasedAnalogies[:20]:\n",
    "    print(score, analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01438105878072613"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([score for (score,_,_) in softDebiasedAnalogies])\n",
    "np.mean([score for (score,_,_) in softDebiasedAnalogies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalTargets, evalAttrs = load_eval_terms(CFG.vocabPath, CFG.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_evaluation(embeddings, targets, attributes):\n",
    "\ttargets_eval = []\n",
    "\tfor targetSet in targets:\n",
    "\t\tfor target in targetSet:\n",
    "\t\t\tfor attributeSet in attributes:\n",
    "\t\t\t\ttargets_eval.append(_unary_s(embeddings, target, attributeSet))\n",
    "\tm_score = np.mean(targets_eval)\n",
    "\treturn m_score, targets_eval\n",
    "\n",
    "def _unary_s(embeddings, target, attributes):\n",
    "\treturn np.mean([ spatial.distance.cosine(embeddings[target], embeddings[ai]) for ai in attributes ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([['manager', 'executive', 'doctor', 'lawyer', 'programmer', 'scientist', 'soldier', 'supervisor', 'rancher', 'janitor', 'firefighter', 'officer'], ['secretary', 'nurse', 'clerk', 'artist', 'homemaker', 'dancer', 'singer', 'librarian', 'maid', 'hairdresser', 'stylist', 'receptionist', 'counselor']])\n"
     ]
    }
   ],
   "source": [
    "print(evalAttrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'she'], ['his', 'hers'], ['son', 'daughter'], ['father', 'mother'], ['male', 'female'], ['boy', 'girl'], ['uncle', 'aunt']]\n"
     ]
    }
   ],
   "source": [
    "print(evalTargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased Evaluation Results\n",
      "Biased MAC: 0.4497686945496877\n"
     ]
    }
   ],
   "source": [
    "print(\"Biased Evaluation Results\")\n",
    "biasedMAC, biasedDistribution = multiclass_evaluation(embedding_dict, evalTargets, evalAttrs)\n",
    "print(\"Biased MAC:\", biasedMAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARD Debiased Evaluation Results\n",
      "HARD MAC: 0.45832520102695906\n",
      "HARD Debiased Cosine difference t-test 0.00045344315625030706\n"
     ]
    }
   ],
   "source": [
    "print(\"HARD Debiased Evaluation Results\")\n",
    "debiasedMAC, debiasedDistribution = multiclass_evaluation(new_hard_word_vectors, evalTargets, evalAttrs)\n",
    "print(\"HARD MAC:\", debiasedMAC)\n",
    "\n",
    "statistics, pvalue = ttest_rel(biasedDistribution, debiasedDistribution)\n",
    "print(\"HARD Debiased Cosine difference t-test\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/\" + CFG.outprefix + \"_statistics-hard.csv\", \"w\")\n",
    "f.write(\"Biased MAC,Debiased MAC,P-Value\\n\")\n",
    "f.write(str(biasedMAC) + \",\" +  str(debiasedMAC) + \",\" + str(pvalue) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFT Debiased Evaluation Results\n",
      "soft MAC: 0.6830000908872161\n",
      "soft Debiased Cosine difference t-test 1.3434180799446991e-34\n"
     ]
    }
   ],
   "source": [
    "print(\"SOFT Debiased Evaluation Results\")\n",
    "debiasedMAC, debiasedDistribution = multiclass_evaluation(new_soft_word_vectors, evalTargets, evalAttrs)\n",
    "print(\"soft MAC:\", debiasedMAC)\n",
    "\n",
    "statistics, pvalue = ttest_rel(biasedDistribution, debiasedDistribution)\n",
    "print(\"soft Debiased Cosine difference t-test\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/\" + CFG.outprefix + \"_statistics-soft.csv\", \"w\")\n",
    "f.write(\"Biased MAC,Debiased MAC,P-Value\\n\")\n",
    "f.write(str(biasedMAC) + \",\" +  str(debiasedMAC) + \",\" + str(pvalue) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_soft Debiased Evaluation Results\n",
      "new_soft MAC: 0.9847870854266257\n",
      "new_soft Debiased Cosine difference t-test 5.837494303555764e-39\n"
     ]
    }
   ],
   "source": [
    "print(\"new_soft Debiased Evaluation Results\")\n",
    "debiasedMAC, debiasedDistribution = multiclass_evaluation(new_new_soft_word_vectors, evalTargets, evalAttrs)\n",
    "print(\"new_soft MAC:\", debiasedMAC)\n",
    "\n",
    "statistics, pvalue = ttest_rel(biasedDistribution, debiasedDistribution)\n",
    "print(\"new_soft Debiased Cosine difference t-test\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/\" + CFG.outprefix + \"_statistics-new-soft.csv\", \"w\")\n",
    "f.write(\"Biased MAC,Debiased MAC,P-Value\\n\")\n",
    "f.write(str(biasedMAC) + \",\" +  str(debiasedMAC) + \",\" + str(pvalue) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = [item[0] for item in evalTargets]\n",
    "female_words = [item[1] for item in evalTargets]\n",
    "attribute_female_professions = list(evalAttrs)[1]\n",
    "attribute_male_professions = list(evalAttrs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_biased_vectors = convert_legacy_to_keyvec(embedding_dict)\n",
    "# model = wefe.word_embedding_model.WordEmbeddingModel(gensim_biased_vectors, \"biased_vectors_model\")\n",
    "\n",
    "# query = wefe.query.Query([male_words, female_words], [attribute_male_professions, attribute_female_professions], ['male words', 'female words'], ['male professions', 'female professions'])\n",
    "\n",
    "# weat = wefe.metrics.WEAT()\n",
    "# result = weat.run_query(query, model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_hard_debiased_vectors = convert_legacy_to_keyvec(new_hard_word_vectors)\n",
    "# model = wefe.word_embedding_model.WordEmbeddingModel(gensim_hard_debiased_vectors, \"hard_debiased_vectors_model\")\n",
    "\n",
    "# query = wefe.query.Query([male_words, female_words], [attribute_male_professions, attribute_female_professions], ['male words', 'female words'], ['male professions', 'female professions'])\n",
    "\n",
    "# weat = wefe.metrics.WEAT()\n",
    "# result = weat.run_query(query, model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_soft_debiased_vectors = convert_legacy_to_keyvec(new_soft_word_vectors)\n",
    "# model = wefe.word_embedding_model.WordEmbeddingModel(gensim_soft_debiased_vectors, \"soft_debiased_vectors_model\")\n",
    "\n",
    "# query = wefe.query.Query([male_words, female_words], [attribute_male_professions, attribute_female_professions], ['male words', 'female words'], ['male professions', 'female professions'])\n",
    "\n",
    "# weat = wefe.metrics.WEAT()\n",
    "# result = weat.run_query(query, model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_soft_debiased_vectors = convert_legacy_to_keyvec(new_soft_word_vectors)\n",
    "# model = wefe.word_embedding_model.WordEmbeddingModel(gensim_soft_debiased_vectors, \"soft_debiased_vectors_model\")\n",
    "\n",
    "# query = wefe.query.Query([male_words, female_words], [attribute_male_professions, attribute_female_professions], ['male words', 'female words'], ['male professions', 'female professions'])\n",
    "\n",
    "# weat = wefe.metrics.MAC()\n",
    "# result = weat.run_query(query, model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CFG.stereoset_file) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in list(model.keys()):\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data___ = [item for item in data['data']['intersentence'] if item['bias_type']==CFG.topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = []\n",
    "\n",
    "for item in data___:\n",
    "    data_dict = defaultdict()\n",
    "    data_dict['context'] = item['context']\n",
    "    for item_ in item['sentences']:\n",
    "        if item_['gold_label'] == 'stereotype':\n",
    "            data_dict['stereotype'] = item_['sentence']\n",
    "        elif item_['gold_label'] == 'anti-stereotype':\n",
    "            data_dict['anti-stereotype'] = item_['sentence']\n",
    "    data_.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stereotype Score: 0.5041322314049587\n"
     ]
    }
   ],
   "source": [
    "total_samples = 0\n",
    "stereotypical_samples = 0\n",
    "\n",
    "# Iterate over the data\n",
    "for item in data_:\n",
    "    context = item['context']\n",
    "    stereo = item['stereotype']\n",
    "    antistereo = item['anti-stereotype']\n",
    "\n",
    "    # Calculate sentence embeddings\n",
    "    context_vec = avg_feature_vector(context, new_new_soft_word_vectors, num_features=CFG.embedding_dim)\n",
    "    stereo_vec = avg_feature_vector(stereo, new_new_soft_word_vectors, num_features=CFG.embedding_dim)\n",
    "    antistereo_vec = avg_feature_vector(antistereo, new_new_soft_word_vectors, num_features=CFG.embedding_dim)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    simstereo = spatial.distance.cosine(context_vec, stereo_vec)\n",
    "    simantistereo = spatial.distance.cosine(context_vec, antistereo_vec)\n",
    "\n",
    "    if simstereo > simantistereo:\n",
    "        stereotypical_samples += 1\n",
    "    total_samples += 1\n",
    "\n",
    "# Calculate stereotype score\n",
    "stereotype_score = stereotypical_samples / total_samples\n",
    "print('Stereotype Score:', stereotype_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R106",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
