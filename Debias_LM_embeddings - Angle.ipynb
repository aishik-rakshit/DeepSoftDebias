{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    vocabPath = \"data/gender_attributes_optm.json\"\n",
    "    male_words = \"data/male_word_file.txt\"\n",
    "    female_words = \"data/female_word_file.txt\"\n",
    "    words = \"data/reddit.US.txt.tok.clean.cleanedforw2v_1.w2v\"\n",
    "    male_embedddings = f'{model_name.replace(\"/\", \"-\")}-male-embeddings.npy'\n",
    "    female_embedddings = f'{model_name.replace(\"/\", \"-\")}-female-embeddings.npy'\n",
    "    word_embedddings = f'word_embeddings/{model_name.replace(\"/\", \"-\")}-embeddings.npy'\n",
    "    outprefix = model_name.replace(\"/\", \"-\")+\"-\"+vocabPath.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\".\", \"_\")\n",
    "    stereoset_file = \"data/dev.json\"\n",
    "    mode = \"role\"\n",
    "    subspace_dim = 14\n",
    "    device = \"cuda\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aishik/anaconda3/envs/R106/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.stats import ttest_rel, spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "import wefe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidWord(word):\n",
    "    return all([c.isalpha() for c in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruneWordVecs(wordVecs):\n",
    "    newWordVecs = {}\n",
    "    for word, vec in wordVecs.items():\n",
    "        valid=True\n",
    "        if(not isValidWord(word)):\n",
    "            valid = False\n",
    "        if(valid):\n",
    "            newWordVecs[word] = vec\n",
    "    return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words(w2v_file):\n",
    "    words = []\n",
    "    with open(w2v_file, 'r') as f:\n",
    "        for line in f:\n",
    "            vect = line.strip().rsplit()\n",
    "            word = vect[0]\n",
    "            words.append(word)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = load_words(CFG.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(CFG.model_name).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analogy_templates(json_filepath, mode):\n",
    "\twith open(json_filepath, \"r\") as f:\t\n",
    "\t\tloadedData = json.load(f)\n",
    "\t\treturn loadedData[\"analogy_templates\"][mode]\n",
    "\n",
    "def load_test_terms(json_filepath):\n",
    "\twith open(json_filepath, \"r\") as f:\t\n",
    "\t\tloadedData = json.load(f)\n",
    "\t\treturn loadedData[\"testTerms\"]\n",
    "\n",
    "def load_eval_terms(json_filepath, mode):\n",
    "\twith open(json_filepath, \"r\") as f:\t\n",
    "\t\tloadedData = json.load(f)\n",
    "\t\treturn loadedData[\"eval_targets\"], loadedData[\"analogy_templates\"][mode].values()\n",
    "\n",
    "def load_def_sets(json_filepath):\n",
    "\twith open(json_filepath, \"r\") as f: \n",
    "\t\tloadedData = json.load(f)\n",
    "\t\treturn {i: v for i, v in enumerate(loadedData[\"definite_sets\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Vocabulary\")\n",
    "analogyTemplates = load_analogy_templates(CFG.vocabPath, CFG.mode)\n",
    "defSets = load_def_sets(CFG.vocabPath)\n",
    "testTerms = load_test_terms(CFG.vocabPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'man': ['manager', 'executive', 'doctor', 'lawyer', 'programmer', 'scientist', 'soldier', 'supervisor', 'rancher', 'janitor', 'firefighter', 'officer'], 'woman': ['secretary', 'nurse', 'clerk', 'artist', 'homemaker', 'dancer', 'singer', 'librarian', 'maid', 'hairdresser', 'stylist', 'receptionist', 'counselor']}\n"
     ]
    }
   ],
   "source": [
    "print(analogyTemplates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['he', 'she'], 1: ['his', 'hers'], 2: ['son', 'daughter'], 3: ['father', 'mother'], 4: ['male', 'female'], 5: ['boy', 'girl'], 6: ['uncle', 'aunt']}\n"
     ]
    }
   ],
   "source": [
    "print(defSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral Words ['manager', 'executive', 'doctor', 'lawyer', 'programmer', 'scientist', 'soldier', 'supervisor', 'rancher', 'janitor', 'firefighter', 'officer', 'secretary', 'nurse', 'clerk', 'artist', 'homemaker', 'dancer', 'singer', 'librarian', 'maid', 'hairdresser', 'stylist', 'receptionist', 'counselor']\n"
     ]
    }
   ],
   "source": [
    "neutral_words = []\n",
    "for value in analogyTemplates.values():\n",
    "    neutral_words.extend(value)\n",
    "print(f\"Neutral Words {neutral_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_word_embeddings = model.encode(neutral_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "neutral_embedding_dict = {word: embedding for word, embedding in zip(neutral_words, neutral_word_embeddings)}\n",
    "embedding_dim = neutral_word_embeddings.shape[-1]\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bias_subspace(vocab, def_sets, subspace_dim, embedding_dim):\n",
    "    \"\"\"\n",
    "    Similar to bolukbasi's implementation at\n",
    "    https://github.com/tolga-b/debiaswe/blob/master/debiaswe/debias.py\n",
    "\n",
    "    vocab - dictionary mapping words to embeddings\n",
    "    def_sets - sets of words that represent extremes? of the subspace\n",
    "            we're interested in (e.g. man-woman, boy-girl, etc. for binary gender)\n",
    "    subspace_dim - number of vectors defining the subspace\n",
    "    embedding_dim - dimensions of the word embeddings\n",
    "    \"\"\"\n",
    "    # calculate means of defining sets\n",
    "    means = {}\n",
    "    for k, v in def_sets.items():\n",
    "        wSet = []\n",
    "        for w in v:\n",
    "            try:\n",
    "                wSet.append(vocab[w])\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        set_vectors = np.array(wSet)\n",
    "        means[k] = np.mean(set_vectors, axis=0)\n",
    "\n",
    "    # calculate vectors to perform PCA\n",
    "    matrix = []\n",
    "    for k, v in def_sets.items():\n",
    "        wSet = []\n",
    "        for w in v:\n",
    "            try:\n",
    "                wSet.append(vocab[w])\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        set_vectors = np.array(wSet)\n",
    "        diffs = set_vectors - means[k]\n",
    "        matrix.append(diffs)\n",
    "\n",
    "    matrix = np.concatenate(matrix)\n",
    "\n",
    "    pca = PCA(n_components=subspace_dim)\n",
    "    pca.fit(matrix)\n",
    "\n",
    "    return pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CFG.male_words, 'r') as file:\n",
    "    # Read the file\n",
    "    data = file.read()\n",
    "\n",
    "# Split the file into words\n",
    "male_words = data.split()\n",
    "with open(CFG.female_words, 'r') as file:\n",
    "    # Read the file\n",
    "    data = file.read()\n",
    "\n",
    "# Split the file into words\n",
    "female_words = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = np.load(CFG.word_embedddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {word: embedding for word, embedding in zip(words, word_embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = pruneWordVecs(embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['he', 'she'], 1: ['his', 'hers'], 2: ['son', 'daughter'], 3: ['father', 'mother'], 4: ['male', 'female'], 5: ['boy', 'girl'], 6: ['uncle', 'aunt']}\n"
     ]
    }
   ],
   "source": [
    "print(defSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 1024)\n"
     ]
    }
   ],
   "source": [
    "subspace = identify_bias_subspace(embedding_dict, defSets, CFG.subspace_dim, embedding_dim)[:CFG.subspace_dim]\n",
    "print(subspace.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_subspace(vector, subspace):\n",
    "    v_b = np.zeros_like(vector)\n",
    "    for component in subspace:\n",
    "        v_b += np.dot(vector.transpose(), component) * component\n",
    "    return v_b\n",
    "\n",
    "def normalize(word_vectors):\n",
    "    for k, v in word_vectors.items():\n",
    "        word_vectors[k] = v / np.linalg.norm(v)\n",
    "\n",
    "def neutralize_and_equalize(vocab, words, eq_sets, bias_subspace, embedding_dim):\n",
    "    \"\"\"\n",
    "    vocab - dictionary mapping words to embeddings\n",
    "    words - words to neutralize\n",
    "    eq_sets - set of equality sets\n",
    "    bias_subspace - subspace of bias from identify_bias_subspace\n",
    "    embedding_dim - dimensions of the word embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    if bias_subspace.ndim == 1:\n",
    "        bias_subspace = np.expand_dims(bias_subspace, 0)\n",
    "    elif bias_subspace.ndim != 2:\n",
    "        raise ValueError(\"bias subspace should be either a matrix or vector\")\n",
    "\n",
    "    new_vocab = vocab.copy()\n",
    "    for w in words:\n",
    "        # get projection onto bias subspace\n",
    "        if w in vocab:\n",
    "            v = vocab[w]\n",
    "            v_b = project_onto_subspace(v, bias_subspace)\n",
    "\n",
    "            new_v = (v - v_b) / np.linalg.norm(v - v_b)\n",
    "            #print np.linalg.norm(new_v)\n",
    "            # update embedding\n",
    "            new_vocab[w] = new_v\n",
    "\n",
    "    normalize(new_vocab)\n",
    "\n",
    "    for eq_set in eq_sets:\n",
    "        mean = np.zeros((embedding_dim,))\n",
    "\n",
    "        #Make sure the elements in the eq sets are valid\n",
    "        cleanEqSet = []\n",
    "        for w in eq_set:\n",
    "            try:\n",
    "                _ = new_vocab[w]\n",
    "                cleanEqSet.append(w)\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "        for w in cleanEqSet:\n",
    "            mean += new_vocab[w]\n",
    "        mean /= float(len(cleanEqSet))\n",
    "\n",
    "        mean_b = project_onto_subspace(mean, bias_subspace)\n",
    "        upsilon = mean - mean_b\n",
    "\n",
    "        for w in cleanEqSet:\n",
    "            v = new_vocab[w]\n",
    "            v_b = project_onto_subspace(v, bias_subspace)\n",
    "\n",
    "            frac = (v_b - mean_b) / np.linalg.norm(v_b - mean_b)\n",
    "            new_v = upsilon + np.sqrt(1 - np.sum(np.square(upsilon))) * frac\n",
    "\n",
    "            new_vocab[w] = new_v\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hard_word_vectors = neutralize_and_equalize(embedding_dict, neutral_words, defSets.values(), subspace, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_402918/1368562009.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  Neutrals = torch.tensor([vocab[w] for w in words]).float().t()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss @ Epoch #0: tensor(24761102., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #1: tensor(22367928., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #2: tensor(20209240., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #3: tensor(18262498., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #4: tensor(16507548., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #5: tensor(14925551., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #6: tensor(13500064., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #7: tensor(12216156., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #8: tensor(11060620., grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #9: tensor(10020737., grad_fn=<AddBackward0>)\n",
      "Optimization Completed, normalizing vector transform\n"
     ]
    }
   ],
   "source": [
    "def equalize_and_soften(vocab, words, eq_sets, bias_subspace, embedding_dim, l=0.2, verbose=True):\n",
    "    vocabIndex, vocabVectors = zip(*vocab.items())\n",
    "    vocabIndex = {i:label for i, label in enumerate(vocabIndex)}\n",
    "    Neutrals = torch.tensor([vocab[w] for w in words]).float().t()\n",
    "\n",
    "    Words = torch.tensor(vocabVectors).float().t()\n",
    "\n",
    "    # perform SVD on W to reduce memory and computational costs\n",
    "    # based on suggestions in supplementary material of Bolukbasi et al.\n",
    "    u, s, _ = torch.svd(Words)\n",
    "    s = torch.diag(s)\n",
    "\n",
    "    # precompute\n",
    "    t1 = s.mm(u.t())\n",
    "    t2 = u.mm(s)\n",
    "\n",
    "    Transform = torch.randn(embedding_dim, embedding_dim).float()\n",
    "    BiasSpace = torch.tensor(bias_subspace).reshape(embedding_dim, -1).float()\n",
    "\n",
    "    Neutrals.requires_grad = False\n",
    "    Words.requires_grad = False\n",
    "    BiasSpace.requires_grad = False\n",
    "    Transform.requires_grad = True\n",
    "\n",
    "    epochs = 10\n",
    "    optimizer = torch.optim.SGD([Transform], lr=0.000001, momentum=0.0)\n",
    "\n",
    "    for i in range(0, epochs):\n",
    "        TtT = torch.mm(Transform.t(), Transform)\n",
    "        norm1 = (t1.mm(TtT - torch.eye(embedding_dim)).mm(t2)).norm(p=2)\n",
    "\n",
    "        norm2 = (Neutrals.t().mm(TtT).mm(BiasSpace)).norm(p=2)\n",
    "\n",
    "        loss = norm1 + l * norm2\n",
    "        norm1 = None\n",
    "        norm2 = None\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Loss @ Epoch #\" + str(i) + \":\", loss)\n",
    "\n",
    "    if(verbose):\n",
    "        print(\"Optimization Completed, normalizing vector transform\")\n",
    "\n",
    "    debiasedVectors = {}\n",
    "    for i, w in enumerate(Words.t()):\n",
    "        transformedVec = torch.mm(Transform, w.view(-1, 1))\n",
    "        debiasedVectors[vocabIndex[i]] = ( transformedVec / transformedVec.norm(p=2) ).detach().numpy().flatten()\n",
    "\n",
    "    return debiasedVectors\n",
    "\n",
    "new_soft_word_vectors = equalize_and_soften(embedding_dict, neutral_words, defSets.values(), subspace, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `equalize_and_soften` function in Python is designed to debias word embeddings. It takes a vocabulary of words with their vector representations, a list of neutral words, a bias subspace, and the dimension of the embeddings as inputs.\n",
    "\n",
    "The function first prepares the data and performs Singular Value Decomposition (SVD) on the word embeddings to reduce computational costs. It then initializes a transformation matrix and a tensor representing the bias subspace.\n",
    "\n",
    "The function uses Stochastic Gradient Descent (SGD) to optimize the transformation matrix over several epochs. The loss function it minimizes consists of two parts: one measures the difference between the dot product of the transformed word embeddings and the identity matrix, and the other measures the projection of the transformed neutral words onto the bias subspace.\n",
    "\n",
    "After the optimization, the function applies the learned transformation to each word in the vocabulary, normalizes the transformed vectors, and returns a dictionary mapping each word to its debiased vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def equalize_and_soften(vocab, words, eq_sets, bias_subspace, embedding_dim, l=0.2, verbose=True):\n",
    "#     vocabIndex, vocabVectors = zip(*vocab.items())\n",
    "#     vocabIndex = {i:label for i, label in enumerate(vocabIndex)}\n",
    "#     Neutrals = torch.tensor([vocab[w] for w in words]).float().t()\n",
    "\n",
    "#     Words = torch.tensor(vocabVectors).float().t()\n",
    "\n",
    "#     # perform SVD on W to reduce memory and computational costs\n",
    "#     # based on suggestions in supplementary material of Bolukbasi et al.\n",
    "#     u, s, _ = torch.svd(Words)\n",
    "#     s = torch.diag(s)\n",
    "\n",
    "#     # precompute\n",
    "#     t1 = s.mm(u.t())\n",
    "#     t2 = u.mm(s)\n",
    "\n",
    "#     Transform = torch.randn(embedding_dim, embedding_dim).float()\n",
    "#     BiasSpace = torch.tensor(bias_subspace).reshape(embedding_dim, -1).float()\n",
    "\n",
    "#     Neutrals.requires_grad = False\n",
    "#     Words.requires_grad = False\n",
    "#     BiasSpace.requires_grad = False\n",
    "#     Transform.requires_grad = True\n",
    "\n",
    "#     epochs = 50\n",
    "#     optimizer = torch.optim.Adam([Transform], lr=0.001)\n",
    "\n",
    "#     for i in range(0, epochs):\n",
    "#         TtT = torch.mm(Transform.t(), Transform)\n",
    "#         norm1 = (t1.mm(TtT - torch.eye(embedding_dim)).mm(t2)).norm(p=2)\n",
    "\n",
    "#         norm2 = (Neutrals.t().mm(TtT).mm(BiasSpace)).norm(p=2)\n",
    "\n",
    "#         loss = norm1 + l * norm2\n",
    "#         norm1 = None\n",
    "#         norm2 = None\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         if(verbose):\n",
    "#             print(\"Loss @ Epoch #\" + str(i) + \":\", loss)\n",
    "\n",
    "#     if(verbose):\n",
    "#         print(\"Optimization Completed, normalizing vector transform\")\n",
    "\n",
    "#     debiasedVectors = {}\n",
    "#     for i, w in enumerate(Words.t()):\n",
    "#         transformedVec = torch.mm(Transform, w.view(-1, 1))\n",
    "#         debiasedVectors[vocabIndex[i]] = ( transformedVec / transformedVec.norm(p=2) ).detach().numpy().flatten()\n",
    "\n",
    "#     return debiasedVectors\n",
    "\n",
    "# new_soft_word_vectors = equalize_and_soften(embedding_dict, neutral_words, defSets.values(), subspace, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss @ Epoch #0: tensor(23362.4082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #1: tensor(7173.9092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #2: tensor(4135.3350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #3: tensor(6654.7422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #4: tensor(8083.3823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #5: tensor(6537.8203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #6: tensor(4231.2832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #7: tensor(3051.9963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #8: tensor(3050.5352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #9: tensor(3155.4119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #10: tensor(2825.5979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #11: tensor(2353.5088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #12: tensor(2125.3633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #13: tensor(2105.8354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #14: tensor(2007.8945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #15: tensor(1734.9020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #16: tensor(1455.1890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #17: tensor(1322.7810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #18: tensor(1295.6324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #19: tensor(1243.2721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #20: tensor(1120.6290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #21: tensor(987.9957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #22: tensor(903.1230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #23: tensor(858.2192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #24: tensor(819.6555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #25: tensor(778.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #26: tensor(744.7539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #27: tensor(715.1888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #28: tensor(672.4763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #29: tensor(614.5728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #30: tensor(563.5447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #31: tensor(537.4171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #32: tensor(532.4566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #33: tensor(530.4967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #34: tensor(515.3304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #35: tensor(488.5269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #36: tensor(463.3585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #37: tensor(447.1264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #38: tensor(437.4362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #39: tensor(428.4599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #40: tensor(416.9783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #41: tensor(404.4676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #42: tensor(393.8802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #43: tensor(386.1534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #44: tensor(380.0515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #45: tensor(373.7146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #46: tensor(366.2463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #47: tensor(358.2449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #48: tensor(350.8666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss @ Epoch #49: tensor(344.7282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Optimization Completed, normalizing vector transform\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "class TransformNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TransformNet, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def equalize_and_soften(vocab, words, eq_sets, bias_subspace, embedding_dim, l=0.2, verbose=True):\n",
    "    vocabIndex, vocabVectors = zip(*vocab.items())\n",
    "    vocabIndex = {i:label for i, label in enumerate(vocabIndex)}\n",
    "    Neutrals = torch.tensor([vocab[w] for w in words]).float().t().to(\"cuda\")\n",
    "\n",
    "    Words = torch.tensor(vocabVectors).float().t().to(\"cuda\")\n",
    "\n",
    "    Transform = TransformNet(embedding_dim, embedding_dim).to(\"cuda\")\n",
    "    BiasSpace = torch.tensor(bias_subspace).reshape(embedding_dim, -1).float().to(\"cuda\")\n",
    "\n",
    "    Neutrals.requires_grad = False\n",
    "    Words.requires_grad = False\n",
    "    BiasSpace.requires_grad = False\n",
    "\n",
    "    epochs = 50\n",
    "    optimizer = optim.Adam(Transform.parameters(), lr=0.001)\n",
    "\n",
    "    identity_matrix = torch.eye(embedding_dim).to(\"cuda\")\n",
    "\n",
    "    Words = Words.t()\n",
    "    for i in range(0, epochs):\n",
    "        transformed_words = Transform(Words)\n",
    "        norm1 = torch.norm(torch.matmul(transformed_words.t(), transformed_words) - identity_matrix)\n",
    "\n",
    "        norm2 = torch.norm(torch.matmul(Neutrals.t(), transformed_words.t()))\n",
    "\n",
    "        loss = norm1 + l * norm2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Loss @ Epoch #\" + str(i) + \":\", loss)\n",
    "\n",
    "    if(verbose):\n",
    "        print(\"Optimization Completed, normalizing vector transform\")\n",
    "\n",
    "    debiasedVectors = {}\n",
    "    for i, w in enumerate(Words):\n",
    "        transformedVec = Transform(w.view(1, -1))\n",
    "        debiasedVectors[vocabIndex[i]] = ( transformedVec / transformedVec.norm(p=2) ).detach().cpu().numpy().flatten()\n",
    "\n",
    "    return debiasedVectors\n",
    "\n",
    "new_soft_word_vectors = equalize_and_soften(embedding_dict, neutral_words, defSets.values(), subspace, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python code defines a class `TransformNet` that inherits from PyTorch's `nn.Module`. It represents a simple neural network with a single linear transformation layer.\n",
    "\n",
    "The `equalize_and_soften` function debiases word embeddings. It prepares the data, initializes a transformation network and a bias tensor, and optimizes the transformation network using Adam optimizer over several epochs. The loss function it minimizes consists of two parts: one measures the difference between the dot product of the transformed word embeddings and the identity matrix, and the other measures the projection of the transformed neutral words onto the bias subspace.\n",
    "\n",
    "After the optimization, the function applies the learned transformation to each word in the vocabulary, normalizes the transformed vectors, and returns a dictionary mapping each word to its debiased vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebiasedSentenceTransformer():\n",
    "    def __init__(self, model, debiasModel):\n",
    "        self.model = model\n",
    "        self.debias_model = debiasModel\n",
    "\n",
    "    def encode(self, word):\n",
    "        sentence_embedding = self.model.encode(word)\n",
    "        sentence_embedding = np.array(sentence_embedding)\n",
    "        transformedVec = self.debias_model(word.view(1, -1))\n",
    "        debiased_vec = ( transformedVec / transformedVec.norm(p=2) ).detach().cpu().numpy().flatten()\n",
    "        return debiased_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoredAnalogyAnswers(a,b,x, keyedVecs, thresh=12.5):\n",
    "\twords = [w for w in keyedVecs.key_to_index.keys() if np.linalg.norm(np.array(keyedVecs[w])-np.array(keyedVecs[x])) < thresh]\n",
    "\n",
    "\tdef cos(a,b,x,y):\n",
    "\t\taVec = np.array(keyedVecs[a])\n",
    "\t\tbVec = np.array(keyedVecs[b])\n",
    "\t\txVec = np.array(keyedVecs[x])\n",
    "\t\tyVec = np.array(keyedVecs[y])\n",
    "\t\tnumerator = (aVec-bVec).dot(xVec-yVec)\n",
    "\t\tdenominator = np.linalg.norm(aVec-bVec)*np.linalg.norm(xVec-yVec)\n",
    "\t\treturn numerator/(denominator if denominator != 0 else 1e-6)\n",
    "\n",
    "\treturn sorted([(cos(a,b,x,y), a,b,x,y) for y in words], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAnalogies(analogyTemplates, keyedVecs):\n",
    "\texpandedAnalogyTemplates = []\n",
    "\tfor A, stereotypes in analogyTemplates.items():\n",
    "\t\tfor B, _ in analogyTemplates.items():\n",
    "\t\t\tif(A != B):\n",
    "\t\t\t\tfor stereotype in stereotypes:\n",
    "\t\t\t\t\texpandedAnalogyTemplates.append([A, stereotype, B])\n",
    "\n",
    "\tanalogies = []\n",
    "\toutputGroups = []\n",
    "\tfor a,b,x in expandedAnalogyTemplates:\n",
    "\t\toutputs = scoredAnalogyAnswers(a,b,x,keyedVecs)\n",
    "\t\tformattedOutput = []\n",
    "\t\t\n",
    "\t\tfor score, a_w, b_w, x_w, y_w in outputs:\n",
    "\t\t\t\n",
    "\t\t\tanalogy = str(a_w) + \" is to \" + str(b_w) + \" as \" + str(x_w) + \" is to \" + str(y_w)\n",
    "\t\t\tanalogyRaw = [a_w, b_w, x_w, y_w]\n",
    "\t\t\tanalogies.append([score, analogy, analogyRaw])\n",
    "\t\t\tformattedOutput.append([score, analogy, analogyRaw])\n",
    "\t\toutputGroups.append(formattedOutput)\n",
    "\n",
    "\tanalogies = sorted(analogies, key=lambda x:-x[0])\n",
    "\treturn analogies, outputGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_legacy_to_keyvec(legacy_w2v):\n",
    "    dim = len(legacy_w2v[list(legacy_w2v.keys())[0]])\n",
    "    vectors = Word2VecKeyedVectors(dim)\n",
    "\n",
    "    ws = []\n",
    "    vs = []\n",
    "\n",
    "    for word, vect in legacy_w2v.items():\n",
    "        ws.append(word)\n",
    "        vs.append(vect)\n",
    "        assert(len(vect) == dim)\n",
    "    vectors.add_vectors(ws, vs, replace=True)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'man': ['manager', 'executive', 'doctor', 'lawyer', 'programmer', 'scientist', 'soldier', 'supervisor', 'rancher', 'janitor', 'firefighter', 'officer'], 'woman': ['secretary', 'nurse', 'clerk', 'artist', 'homemaker', 'dancer', 'singer', 'librarian', 'maid', 'hairdresser', 'stylist', 'receptionist', 'counselor']}\n"
     ]
    }
   ],
   "source": [
    "print(analogyTemplates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasedAnalogies, biasedAnalogyGroups = generateAnalogies(analogyTemplates, convert_legacy_to_keyvec(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardDebiasedAnalogies, hardDebiasedAnalogyGroups = generateAnalogies(analogyTemplates, convert_legacy_to_keyvec(new_hard_word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "softDebiasedAnalogies, softDebiasedAnalogyGroups = generateAnalogies(analogyTemplates, convert_legacy_to_keyvec(new_soft_word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeAnalogies(analogies, path):\n",
    "    f = open(path, \"w\")\n",
    "    f.write(\"Score,Analogy\\n\")\n",
    "    for score, analogy, raw in analogies:\n",
    "        f.write(str(score) + \",\" + str(analogy) + \",\" + str(raw) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "def writeGroupAnalogies(groups, path):\n",
    "    f = open(path, \"w\")\n",
    "    f.write(\"Score,Analogy\\n\")\n",
    "    for analogies in groups:\n",
    "        for score, analogy, raw in analogies:\n",
    "            f.write(str(score) + \",\" + str(analogy) + \",\" + str(raw) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeAnalogies(biasedAnalogies, \"output/\" + CFG.outprefix + \"_biasedAnalogiesOut.csv\")\n",
    "writeGroupAnalogies(biasedAnalogyGroups, \"output/\" + CFG.outprefix + \"_biasedAnalogiesOut_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeAnalogies(hardDebiasedAnalogies, \"output/\" + CFG.outprefix + \"_hardDebiasedAnalogiesOut.csv\")\n",
    "writeGroupAnalogies(hardDebiasedAnalogyGroups, \"output/\" + CFG.outprefix + \"_hardDebiasedAnalogiesOut_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeAnalogies(softDebiasedAnalogies, \"output/\" + CFG.outprefix + \"_softDebiasedAnalogiesOut.csv\")\n",
    "writeGroupAnalogies(softDebiasedAnalogyGroups, \"output/\" + CFG.outprefix + \"_softDebiasedAnalogiesOut_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58663553 woman is to maid as man is to maid\n",
      "0.57359684 woman is to artist as man is to artist\n",
      "0.5720325 woman is to secretary as man is to secretary\n",
      "0.56268376 man is to firefighter as woman is to firefighter\n",
      "0.54958075 man is to rancher as woman is to rancher\n",
      "0.54686826 woman is to hairdresser as man is to hairdresser\n",
      "0.5389029 man is to supervisor as woman is to supervisor\n",
      "0.5362094 woman is to receptionist as man is to wobbles\n",
      "0.52826476 woman is to homemaker as man is to homemaker\n",
      "0.52508116 woman is to receptionist as man is to receptionist\n"
     ]
    }
   ],
   "source": [
    "for score, analogy, _ in biasedAnalogies[:10]:\n",
    "    print(score, analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00929687691981717"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([score for (score,_,_) in biasedAnalogies])\n",
    "np.mean([score for (score,_,_) in biasedAnalogies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59924316 woman is to maid as man is to maid\n",
      "0.5832432 woman is to artist as man is to artist\n",
      "0.5671364 woman is to secretary as man is to secretary\n",
      "0.55536425 man is to rancher as woman is to rancher\n",
      "0.55404055 man is to firefighter as woman is to firefighter\n",
      "0.55226827 woman is to hairdresser as man is to hairdresser\n",
      "0.54285866 man is to supervisor as woman is to supervisor\n",
      "0.5362576 woman is to receptionist as man is to wobbles\n",
      "0.53297687 woman is to homemaker as man is to homemaker\n",
      "0.5320646 woman is to receptionist as man is to receptionist\n",
      "0.5313066 man is to executive as woman is to executive\n",
      "0.5263474 man is to janitor as woman is to janitor\n",
      "0.5255713 man is to manager as woman is to manager\n",
      "0.5153393 woman is to singer as man is to singer\n",
      "0.5090588 man is to doctor as woman is to doctor\n",
      "0.50003546 woman is to counselor as man is to counselor\n",
      "0.49472508 man is to programmer as woman is to programmer\n",
      "0.4901575 man is to lawyer as woman is to lawyer\n",
      "0.48815748 woman is to stylist as man is to stylist\n",
      "0.48513496 woman is to librarian as man is to librarian\n"
     ]
    }
   ],
   "source": [
    "for score, analogy, _ in hardDebiasedAnalogies[:20]:\n",
    "    print(score, analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49156445 woman is to receptionist as man is to wobbles\n",
      "0.4901132 woman is to receptionist as man is to receptionist\n",
      "0.48717773 man is to executive as woman is to executive\n",
      "0.48671648 woman is to secretary as man is to secretary\n",
      "0.48309934 woman is to maid as man is to maid\n",
      "0.47835666 man is to doctor as woman is to doctor\n",
      "0.47823337 woman is to artist as man is to artist\n",
      "0.47754997 man is to janitor as woman is to janitor\n",
      "0.47683594 man is to programmer as woman is to programmer\n",
      "0.47014034 man is to manager as woman is to manager\n",
      "0.4689608 man is to rancher as woman is to rancher\n",
      "0.4660783 woman is to hairdresser as man is to hairdresser\n",
      "0.46498796 man is to firefighter as woman is to firefighter\n",
      "0.45899945 woman is to singer as man is to singer\n",
      "0.4541491 woman is to receptionist as man is to cock\n",
      "0.45385262 woman is to dancer as man is to dancer\n",
      "0.44955054 man is to executive as woman is to principled\n",
      "0.4481435 man is to lawyer as woman is to lawyer\n",
      "0.4474094 woman is to homemaker as man is to homemaker\n",
      "0.4417501 woman is to counselor as man is to counselor\n"
     ]
    }
   ],
   "source": [
    "for score, analogy, _ in softDebiasedAnalogies[:20]:\n",
    "    print(score, analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.042506397704258195"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([score for (score,_,_) in softDebiasedAnalogies])\n",
    "np.mean([score for (score,_,_) in softDebiasedAnalogies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalTargets, evalAttrs = load_eval_terms(CFG.vocabPath, CFG.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_evaluation(embeddings, targets, attributes):\n",
    "\ttargets_eval = []\n",
    "\tfor targetSet in targets:\n",
    "\t\tfor target in targetSet:\n",
    "\t\t\tfor attributeSet in attributes:\n",
    "\t\t\t\ttargets_eval.append(_unary_s(embeddings, target, attributeSet))\n",
    "\tm_score = np.mean(targets_eval)\n",
    "\treturn m_score, targets_eval\n",
    "\n",
    "def _unary_s(embeddings, target, attributes):\n",
    "\treturn np.mean([ spatial.distance.cosine(embeddings[target], embeddings[ai]) for ai in attributes ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([['manager', 'executive', 'doctor', 'lawyer', 'programmer', 'scientist', 'soldier', 'supervisor', 'rancher', 'janitor', 'firefighter', 'officer'], ['secretary', 'nurse', 'clerk', 'artist', 'homemaker', 'dancer', 'singer', 'librarian', 'maid', 'hairdresser', 'stylist', 'receptionist', 'counselor']])\n"
     ]
    }
   ],
   "source": [
    "print(evalAttrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'she'], ['his', 'hers'], ['son', 'daughter'], ['father', 'mother'], ['male', 'female'], ['boy', 'girl'], ['uncle', 'aunt']]\n"
     ]
    }
   ],
   "source": [
    "print(evalTargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fawn',\n",
       " 'gah',\n",
       " 'sowell',\n",
       " 'woods',\n",
       " 'mechanicus',\n",
       " 'spiders',\n",
       " 'hanging',\n",
       " 'woody',\n",
       " 'comically',\n",
       " 'localized',\n",
       " 'movesets',\n",
       " 'disobeying',\n",
       " 'canes',\n",
       " 'scold',\n",
       " 'praze',\n",
       " 'nukem',\n",
       " 'originality',\n",
       " 'refunding',\n",
       " 'unnecessarily',\n",
       " 'crossbar',\n",
       " 'hermann',\n",
       " 'midgame',\n",
       " 'evangelical',\n",
       " 'donger',\n",
       " 'stipulate',\n",
       " 'eugenics',\n",
       " 'appropriation',\n",
       " 'closers',\n",
       " 'scandanavia',\n",
       " 'politician',\n",
       " 'broward',\n",
       " 'bringing',\n",
       " 'jrpg',\n",
       " 'wooded',\n",
       " 'grueling',\n",
       " 'wooden',\n",
       " 'wednesday',\n",
       " 'circuitry',\n",
       " 'crotch',\n",
       " 'asami',\n",
       " 'stereotypical',\n",
       " 'immunities',\n",
       " 'guardsmen',\n",
       " 'rlm',\n",
       " 'thrace',\n",
       " 'gaskets',\n",
       " 'punchable',\n",
       " 'complainers',\n",
       " 'scraper',\n",
       " 'targs',\n",
       " 'feasibility',\n",
       " 'miniatures',\n",
       " 'mortgages',\n",
       " 'imploding',\n",
       " 'sustaining',\n",
       " 'consenting',\n",
       " 'scraped',\n",
       " 'inanimate',\n",
       " 'errors',\n",
       " 'tiered',\n",
       " 'cyprus',\n",
       " 'cooking',\n",
       " 'hp',\n",
       " 'warmongering',\n",
       " 'usenet',\n",
       " 'vassals',\n",
       " 'lmq',\n",
       " 'numeral',\n",
       " 'hallucinating',\n",
       " 'succumb',\n",
       " 'shocks',\n",
       " 'widget',\n",
       " 'crouch',\n",
       " 'chins',\n",
       " 'brainwashed',\n",
       " 'affiliates',\n",
       " 'ching',\n",
       " 'china',\n",
       " 'affiliated',\n",
       " 'chino',\n",
       " 'confronts',\n",
       " 'chink',\n",
       " 'natured',\n",
       " 'quart',\n",
       " 'kids',\n",
       " 'uplifting',\n",
       " 'kumbaya',\n",
       " 'controversy',\n",
       " 'kidd',\n",
       " 'natures',\n",
       " 'neurologist',\n",
       " 'spotty',\n",
       " 'climber',\n",
       " 'appropriately',\n",
       " 'topography',\n",
       " 'projection',\n",
       " 'wikimedia',\n",
       " 'lengthen',\n",
       " 'stern',\n",
       " 'prodigy',\n",
       " 'quentyn',\n",
       " 'dnd',\n",
       " 'dnf',\n",
       " 'dna',\n",
       " 'dnc',\n",
       " 'dnb',\n",
       " 'catchy',\n",
       " 'insecurity',\n",
       " 'abbreviations',\n",
       " 'dni',\n",
       " 'cannibal',\n",
       " 'sidebars',\n",
       " 'music',\n",
       " 'therefore',\n",
       " 'dns',\n",
       " 'distortions',\n",
       " 'sermons',\n",
       " 'duals',\n",
       " 'populations',\n",
       " 'yahoo',\n",
       " 'meteorologist',\n",
       " 'rickman',\n",
       " 'expeditionary',\n",
       " 'primeval',\n",
       " 'unpack',\n",
       " 'circumstances',\n",
       " 'intake',\n",
       " 'morally',\n",
       " 'locked',\n",
       " 'locker',\n",
       " 'locket',\n",
       " 'soundbite',\n",
       " 'gpas',\n",
       " 'matilda',\n",
       " 'capsaicin',\n",
       " 'wang',\n",
       " 'wand',\n",
       " 'pints',\n",
       " 'unjust',\n",
       " 'pooper',\n",
       " 'want',\n",
       " 'cookers',\n",
       " 'pinto',\n",
       " 'cocksucker',\n",
       " 'absolute',\n",
       " 'mcmuffin',\n",
       " 'shyvana',\n",
       " 'travel',\n",
       " 'copious',\n",
       " 'playback',\n",
       " 'dared',\n",
       " 'gouge',\n",
       " 'cadence',\n",
       " 'uselessness',\n",
       " 'goober',\n",
       " 'fookin',\n",
       " 'assimilated',\n",
       " 'apocrypha',\n",
       " 'dinosaurs',\n",
       " 'modest',\n",
       " 'kaspersky',\n",
       " 'sentencing',\n",
       " 'pigment',\n",
       " 'opted',\n",
       " 'subplots',\n",
       " 'sickening',\n",
       " 'tulip',\n",
       " 'torts',\n",
       " 'porsches',\n",
       " 'nonsensical',\n",
       " 'welcomed',\n",
       " 'stoicism',\n",
       " 'partnered',\n",
       " 'tillamook',\n",
       " 'airbags',\n",
       " 'boltons',\n",
       " 'activating',\n",
       " 'welcomes',\n",
       " 'fir',\n",
       " 'fip',\n",
       " 'fit',\n",
       " 'fiu',\n",
       " 'lifeline',\n",
       " 'uncoordinated',\n",
       " 'fix',\n",
       " 'secede',\n",
       " 'fib',\n",
       " 'fic',\n",
       " 'fia',\n",
       " 'fig',\n",
       " 'wales',\n",
       " 'fin',\n",
       " 'fil',\n",
       " 'zucker',\n",
       " 'bristle',\n",
       " 'songwriter',\n",
       " 'vouchers',\n",
       " 'twits',\n",
       " 'hypoxia',\n",
       " 'effects',\n",
       " 'sixteen',\n",
       " 'undeveloped',\n",
       " 'saddened',\n",
       " 'defecate',\n",
       " 'whacking',\n",
       " 'barton',\n",
       " 'nerfing',\n",
       " 'neonazis',\n",
       " 'timeout',\n",
       " 'burial',\n",
       " 'telescope',\n",
       " 'romulans',\n",
       " 'allan',\n",
       " 'phoenicians',\n",
       " 'parasites',\n",
       " 'tarasenko',\n",
       " 'novices',\n",
       " 'tricked',\n",
       " 'absolves',\n",
       " 'golem',\n",
       " 'unclos',\n",
       " 'touts',\n",
       " 'whaddya',\n",
       " 'smirk',\n",
       " 'enviroment',\n",
       " 'mason',\n",
       " 'encourage',\n",
       " 'noice',\n",
       " 'adapt',\n",
       " 'unitas',\n",
       " 'dogwhistle',\n",
       " 'size',\n",
       " 'outburst',\n",
       " 'misrepresentations',\n",
       " 'gate',\n",
       " 'abbott',\n",
       " 'stamping',\n",
       " 'gameplan',\n",
       " 'worgen',\n",
       " 'strata',\n",
       " 'pumpkins',\n",
       " 'corrects',\n",
       " 'estimate',\n",
       " 'gameplay',\n",
       " 'universally',\n",
       " 'chlorine',\n",
       " 'strats',\n",
       " 'husbands',\n",
       " 'competes',\n",
       " 'ministries',\n",
       " 'disturbed',\n",
       " 'competed',\n",
       " 'dentures',\n",
       " 'portmanteau',\n",
       " 'loudness',\n",
       " 'chronic',\n",
       " 'chromebooks',\n",
       " 'disfigured',\n",
       " 'fuckwad',\n",
       " 'mesmer',\n",
       " 'kashima',\n",
       " 'kfc',\n",
       " 'chevron',\n",
       " 'discworld',\n",
       " 'seizures',\n",
       " 'olds',\n",
       " 'renovated',\n",
       " 'service',\n",
       " 'reuben',\n",
       " 'needed',\n",
       " 'master',\n",
       " 'critter',\n",
       " 'genesis',\n",
       " 'caitlyn',\n",
       " 'rewards',\n",
       " 'scrapes',\n",
       " 'meltdowns',\n",
       " 'pssh',\n",
       " 'tugs',\n",
       " 'mutilated',\n",
       " 'horrible',\n",
       " 'positively',\n",
       " 'spayed',\n",
       " 'duckworth',\n",
       " 'awacs',\n",
       " 'handcuffs',\n",
       " 'meditative',\n",
       " 'idly',\n",
       " 'regulator',\n",
       " 'idle',\n",
       " 'exclaimed',\n",
       " 'friend',\n",
       " 'moneyless',\n",
       " 'feeling',\n",
       " 'tdkr',\n",
       " 'longs',\n",
       " 'codeine',\n",
       " 'politely',\n",
       " 'spectrum',\n",
       " 'increment',\n",
       " 'worlders',\n",
       " 'thaw',\n",
       " 'urinate',\n",
       " 'zardes',\n",
       " 'foundational',\n",
       " 'affairs',\n",
       " 'wholesome',\n",
       " 'berbers',\n",
       " 'hymen',\n",
       " 'achievement',\n",
       " 'paramedics',\n",
       " 'uncouth',\n",
       " 'vga',\n",
       " 'vgc',\n",
       " 'racers',\n",
       " 'snitch',\n",
       " 'kremlin',\n",
       " 'rhcp',\n",
       " 'concedes',\n",
       " 'committing',\n",
       " 'limitless',\n",
       " 'diminishing',\n",
       " 'cinematic',\n",
       " 'resonates',\n",
       " 'metrics',\n",
       " 'disjointed',\n",
       " 'mouth',\n",
       " 'icloud',\n",
       " 'conceded',\n",
       " 'resonated',\n",
       " 'bradford',\n",
       " 'singer',\n",
       " 'thaad',\n",
       " 'purges',\n",
       " 'ragnarok',\n",
       " 'multiracial',\n",
       " 'tech',\n",
       " 'fugitives',\n",
       " 'ragnaros',\n",
       " 'scream',\n",
       " 'saying',\n",
       " 'dickey',\n",
       " 'teresa',\n",
       " 'padded',\n",
       " 'karmanaut',\n",
       " 'ulcer',\n",
       " 'tempted',\n",
       " 'cheaply',\n",
       " 'thai',\n",
       " 'hounded',\n",
       " 'orleans',\n",
       " 'clicked',\n",
       " 'ajayi',\n",
       " 'rico',\n",
       " 'clicker',\n",
       " 'bliss',\n",
       " 'bullshitter',\n",
       " 'rich',\n",
       " 'rice',\n",
       " 'photographers',\n",
       " 'rica',\n",
       " 'plate',\n",
       " 'plata',\n",
       " 'waaaaaay',\n",
       " 'plato',\n",
       " 'umbilical',\n",
       " 'altogether',\n",
       " 'chyron',\n",
       " 'droning',\n",
       " 'jaguar',\n",
       " 'rectal',\n",
       " 'nicely',\n",
       " 'boarder',\n",
       " 'pretzel',\n",
       " 'patch',\n",
       " 'eyelids',\n",
       " 'ldp',\n",
       " 'superclocked',\n",
       " 'boarded',\n",
       " 'cull',\n",
       " 'heirloom',\n",
       " 'clarified',\n",
       " 'sensitivity',\n",
       " 'deathadder',\n",
       " 'pinot',\n",
       " 'jabari',\n",
       " 'clarifies',\n",
       " 'karate',\n",
       " 'lots',\n",
       " 'lotr',\n",
       " 'irs',\n",
       " 'lotv',\n",
       " 'irv',\n",
       " 'lott',\n",
       " 'xvi',\n",
       " 'irk',\n",
       " 'irl',\n",
       " 'conductive',\n",
       " 'ira',\n",
       " 'irc',\n",
       " 'ire',\n",
       " 'wage',\n",
       " 'redistricting',\n",
       " 'khaleesi',\n",
       " 'extend',\n",
       " 'nature',\n",
       " 'fruits',\n",
       " 'lapping',\n",
       " 'garfunkel',\n",
       " 'wheelbarrow',\n",
       " 'tendons',\n",
       " 'cus',\n",
       " 'tyranny',\n",
       " 'airflow',\n",
       " 'veep',\n",
       " 'veer',\n",
       " 'himalayas',\n",
       " 'sparta',\n",
       " 'heating',\n",
       " 'incense',\n",
       " 'lookin',\n",
       " 'himalayan',\n",
       " 'southeastern',\n",
       " 'eradicate',\n",
       " 'rehash',\n",
       " 'mortified',\n",
       " 'tmnt',\n",
       " 'gopher',\n",
       " 'gypsies',\n",
       " 'basque',\n",
       " 'escalade',\n",
       " 'humming',\n",
       " 'solaire',\n",
       " 'smackdown',\n",
       " 'godhead',\n",
       " 'milquetoast',\n",
       " 'union',\n",
       " 'fri',\n",
       " 'remained',\n",
       " 'fro',\n",
       " 'rgb',\n",
       " 'bothers',\n",
       " 'much',\n",
       " 'wyman',\n",
       " 'destructible',\n",
       " 'progenitor',\n",
       " 'tyr',\n",
       " 'fry',\n",
       " 'tallest',\n",
       " 'toning',\n",
       " 'obese',\n",
       " 'sensationalized',\n",
       " 'retrospect',\n",
       " 'spit',\n",
       " 'davy',\n",
       " 'sycophants',\n",
       " 'doubts',\n",
       " 'stepdad',\n",
       " 'spin',\n",
       " 'wildcat',\n",
       " 'participatory',\n",
       " 'zalman',\n",
       " 'contingencies',\n",
       " 'professionally',\n",
       " 'employ',\n",
       " 'misconstrued',\n",
       " 'reunion',\n",
       " 'k',\n",
       " 'shirou',\n",
       " 'deferring',\n",
       " 'saltiness',\n",
       " 'canoeing',\n",
       " 'majin',\n",
       " 'ditching',\n",
       " 'expat',\n",
       " 'kohl',\n",
       " 'conditioned',\n",
       " 'eighteen',\n",
       " 'hyperdrive',\n",
       " 'conditioner',\n",
       " 'oxymoron',\n",
       " 'insatiable',\n",
       " 'hone',\n",
       " 'memorial',\n",
       " 'honk',\n",
       " 'democracies',\n",
       " 'spews',\n",
       " 'split',\n",
       " 'codename',\n",
       " 'dunkirk',\n",
       " 'boiled',\n",
       " 'effortlessly',\n",
       " 'communally',\n",
       " 'chauvinistic',\n",
       " 'kushner',\n",
       " 'frenchmen',\n",
       " 'kumagawa',\n",
       " 'torpedoes',\n",
       " 'workforce',\n",
       " 'qdoba',\n",
       " 'marched',\n",
       " 'boiler',\n",
       " 'supper',\n",
       " 'torpedoed',\n",
       " 'wcw',\n",
       " 'kamina',\n",
       " 'mentors',\n",
       " 'academic',\n",
       " 'stillness',\n",
       " 'academia',\n",
       " 'goofing',\n",
       " 'corporate',\n",
       " 'massaging',\n",
       " 'plaque',\n",
       " 'outlived',\n",
       " 'bellow',\n",
       " 'absurdities',\n",
       " 'golden',\n",
       " 'antagonists',\n",
       " 'terrorist',\n",
       " 'homogeneity',\n",
       " 'airbag',\n",
       " 'brandish',\n",
       " 'lasso',\n",
       " 'noscript',\n",
       " 'hai',\n",
       " 'upsides',\n",
       " 'hal',\n",
       " 'ham',\n",
       " 'han',\n",
       " 'shump',\n",
       " 'hab',\n",
       " 'espouses',\n",
       " 'had',\n",
       " 'insubordination',\n",
       " 'haf',\n",
       " 'hag',\n",
       " 'keynesians',\n",
       " 'hay',\n",
       " 'haz',\n",
       " 'advantages',\n",
       " 'beloved',\n",
       " 'duffel',\n",
       " 'har',\n",
       " 'has',\n",
       " 'espoused',\n",
       " 'hav',\n",
       " 'ecovillage',\n",
       " 'cocked',\n",
       " 'municipal',\n",
       " 'elders',\n",
       " 'survival',\n",
       " 'wcf',\n",
       " 'unequivocally',\n",
       " 'otherworldly',\n",
       " 'indicative',\n",
       " 'clustered',\n",
       " 'shadow',\n",
       " 'replace',\n",
       " 'degrom',\n",
       " 'masonic',\n",
       " 'alice',\n",
       " 'gangplank',\n",
       " 'cogs',\n",
       " 'festivities',\n",
       " 'kunkka',\n",
       " 'misdemeanors',\n",
       " 'warping',\n",
       " 'beneficial',\n",
       " 'sours',\n",
       " 'crowd',\n",
       " 'crowe',\n",
       " 'czech',\n",
       " 'mosques',\n",
       " 'crown',\n",
       " 'topping',\n",
       " 'proportionally',\n",
       " 'deflection',\n",
       " 'crows',\n",
       " 'choctaw',\n",
       " 'billboard',\n",
       " 'fiduciary',\n",
       " 'hemsworth',\n",
       " 'perchance',\n",
       " 'bottom',\n",
       " 'lockdown',\n",
       " 'inhuman',\n",
       " 'plucked',\n",
       " 'frum',\n",
       " 'syphilis',\n",
       " 'monogamy',\n",
       " 'completly',\n",
       " 'brigaded',\n",
       " 'barcode',\n",
       " 'binder',\n",
       " 'brigades',\n",
       " 'starring',\n",
       " 'mediocrity',\n",
       " 'anagram',\n",
       " 'stokes',\n",
       " 'acok',\n",
       " 'benches',\n",
       " 'kotlin',\n",
       " 'anomalous',\n",
       " 'acog',\n",
       " 'benched',\n",
       " 'stoked',\n",
       " 'aden',\n",
       " 'lemmings',\n",
       " 'infraction',\n",
       " 'marshall',\n",
       " 'honeymoon',\n",
       " 'mba',\n",
       " 'administer',\n",
       " 'beings',\n",
       " 'valleys',\n",
       " 'marshals',\n",
       " 'mbt',\n",
       " 'shoots',\n",
       " 'mbp',\n",
       " 'mbs',\n",
       " 'reciprocity',\n",
       " 'fabric',\n",
       " 'altitude',\n",
       " 'diocese',\n",
       " 'raped',\n",
       " 'rapey',\n",
       " 'grasping',\n",
       " 'wordpress',\n",
       " 'greatness',\n",
       " 'rapes',\n",
       " 'raper',\n",
       " 'avocados',\n",
       " 'tamu',\n",
       " 'dnr',\n",
       " 'denoting',\n",
       " 'budaj',\n",
       " 'verde',\n",
       " 'bronies',\n",
       " 'safeguard',\n",
       " 'duel',\n",
       " 'humbled',\n",
       " 'masquerading',\n",
       " 'arrays',\n",
       " 'urawa',\n",
       " 'smasher',\n",
       " 'smashes',\n",
       " 'complications',\n",
       " 'pesos',\n",
       " 'smashed',\n",
       " 'duet',\n",
       " 'dues',\n",
       " 'passenger',\n",
       " 'tannehill',\n",
       " 'disgrace',\n",
       " 'minaj',\n",
       " 'biden',\n",
       " 'minas',\n",
       " 'ana',\n",
       " 'bourdain',\n",
       " 'decapitation',\n",
       " 'timezone',\n",
       " 'pubmed',\n",
       " 'triangles',\n",
       " 'hijabs',\n",
       " 'slurring',\n",
       " 'eventual',\n",
       " 'crowns',\n",
       " 'pasadena',\n",
       " 'role',\n",
       " 'transgender',\n",
       " 'roll',\n",
       " 'rolo',\n",
       " 'intend',\n",
       " 'palms',\n",
       " 'ointment',\n",
       " 'outage',\n",
       " 'devos',\n",
       " 'transported',\n",
       " 'devon',\n",
       " 'intent',\n",
       " 'smelling',\n",
       " 'variable',\n",
       " 'transporter',\n",
       " 'aragorn',\n",
       " 'dispersed',\n",
       " 'explosions',\n",
       " 'shootout',\n",
       " 'ordination',\n",
       " 'overturned',\n",
       " 'osx',\n",
       " 'childs',\n",
       " 'cincinnati',\n",
       " 'corps',\n",
       " 'whoever',\n",
       " 'oss',\n",
       " 'osu',\n",
       " 'ost',\n",
       " 'osi',\n",
       " 'annnnnd',\n",
       " 'bandits',\n",
       " 'chair',\n",
       " 'machu',\n",
       " 'bitchin',\n",
       " 'amplification',\n",
       " 'baller',\n",
       " 'pell',\n",
       " 'freelance',\n",
       " 'crates',\n",
       " 'crater',\n",
       " 'silencers',\n",
       " 'balled',\n",
       " 'macho',\n",
       " 'oversight',\n",
       " 'tenacious',\n",
       " 'downloading',\n",
       " 'paychecks',\n",
       " 'flatbed',\n",
       " 'jerk',\n",
       " 'iggy',\n",
       " 'olympus',\n",
       " 'choice',\n",
       " 'pele',\n",
       " 'embark',\n",
       " 'gloomy',\n",
       " 'ghostbusters',\n",
       " 'crackdowns',\n",
       " 'exact',\n",
       " 'minute',\n",
       " 'catalonia',\n",
       " 'cooks',\n",
       " 'masturbates',\n",
       " 'skewed',\n",
       " 'bellatrix',\n",
       " 'cooke',\n",
       " 'defaults',\n",
       " 'baconreader',\n",
       " 'skewer',\n",
       " 'masturbated',\n",
       " 'xenophobe',\n",
       " 'righties',\n",
       " 'trails',\n",
       " 'copyrighted',\n",
       " 'heavyweight',\n",
       " 'chopping',\n",
       " 'shirts',\n",
       " 'jameis',\n",
       " 'incrementally',\n",
       " 'bagging',\n",
       " 'headset',\n",
       " 'antwerp',\n",
       " 'celebrated',\n",
       " 'paradoxes',\n",
       " 'crackling',\n",
       " 'celebrates',\n",
       " 'unintentionally',\n",
       " 'drafted',\n",
       " 'oldies',\n",
       " 'climbs',\n",
       " 'honour',\n",
       " 'plucking',\n",
       " 'apis',\n",
       " 'address',\n",
       " 'dwindling',\n",
       " 'plunger',\n",
       " 'benson',\n",
       " 'enroll',\n",
       " 'accomplishes',\n",
       " 'dusty',\n",
       " 'impacted',\n",
       " 'procedurally',\n",
       " 'queue',\n",
       " 'staters',\n",
       " 'accomplished',\n",
       " 'sprouted',\n",
       " 'falcao',\n",
       " 'influx',\n",
       " 'umps',\n",
       " 'randoms',\n",
       " 'betraying',\n",
       " 'undergone',\n",
       " 'working',\n",
       " 'perished',\n",
       " 'pooped',\n",
       " 'blackwater',\n",
       " 'gulags',\n",
       " 'gibbons',\n",
       " 'opposed',\n",
       " 'wank',\n",
       " 'liberalizing',\n",
       " 'ooooooh',\n",
       " 'tundra',\n",
       " 'wvu',\n",
       " 'sierra',\n",
       " 'consoles',\n",
       " 'riders',\n",
       " 'rebounding',\n",
       " 'lowercase',\n",
       " 'opioids',\n",
       " 'titanium',\n",
       " 'originally',\n",
       " 'rutherford',\n",
       " 'abortion',\n",
       " 'harmonious',\n",
       " 'albright',\n",
       " 'following',\n",
       " 'zippers',\n",
       " 'admired',\n",
       " 'diabeetus',\n",
       " 'locke',\n",
       " 'groupme',\n",
       " 'mailboxes',\n",
       " 'parachute',\n",
       " 'locks',\n",
       " 'incremental',\n",
       " 'admires',\n",
       " 'admirer',\n",
       " 'listens',\n",
       " 'septic',\n",
       " 'septim',\n",
       " 'dooms',\n",
       " 'thanking',\n",
       " 'rewatched',\n",
       " 'custer',\n",
       " 'mythos',\n",
       " 'convincingly',\n",
       " 'fueled',\n",
       " 'meddled',\n",
       " 'vulkan',\n",
       " 'brainless',\n",
       " 'egotistical',\n",
       " 'temperate',\n",
       " 'pulley',\n",
       " 'surfing',\n",
       " 'conscious',\n",
       " 'finalize',\n",
       " 'kiko',\n",
       " 'quadrant',\n",
       " 'regressive',\n",
       " 'subdivisions',\n",
       " 'mango',\n",
       " 'swollen',\n",
       " 'skirmish',\n",
       " 'wolves',\n",
       " 'pulled',\n",
       " 'manga',\n",
       " 'impactful',\n",
       " 'colorblindness',\n",
       " 'webpage',\n",
       " 'years',\n",
       " 'professors',\n",
       " 'structuring',\n",
       " 'yearn',\n",
       " 'comedians',\n",
       " 'jif',\n",
       " 'jig',\n",
       " 'ovals',\n",
       " 'overlord',\n",
       " 'jib',\n",
       " 'jin',\n",
       " 'jim',\n",
       " 'troubles',\n",
       " 'faves',\n",
       " 'jiu',\n",
       " 'differentiating',\n",
       " 'antipathy',\n",
       " 'suspension',\n",
       " 'troubled',\n",
       " 'abusive',\n",
       " 'apron',\n",
       " 'recipients',\n",
       " 'civilian',\n",
       " 'dpoy',\n",
       " 'indigenous',\n",
       " 'overpowering',\n",
       " 'drilling',\n",
       " 'sorted',\n",
       " 'lichtenstein',\n",
       " 'shhh',\n",
       " 'nsx',\n",
       " 'frontage',\n",
       " 'hickory',\n",
       " 'materialized',\n",
       " 'didn',\n",
       " 'didi',\n",
       " 'leather',\n",
       " 'fisherman',\n",
       " 'battleships',\n",
       " 'instability',\n",
       " 'quarter',\n",
       " 'quartet',\n",
       " 'honduras',\n",
       " 'vanishingly',\n",
       " 'bursting',\n",
       " 'receipt',\n",
       " 'ayyy',\n",
       " 'sponsor',\n",
       " 'entering',\n",
       " 'honduran',\n",
       " 'salads',\n",
       " 'disasters',\n",
       " 'troll',\n",
       " 'interned',\n",
       " 'seriously',\n",
       " 'trauma',\n",
       " 'internet',\n",
       " 'macedon',\n",
       " 'srssucks',\n",
       " 'complicates',\n",
       " 'curbs',\n",
       " 'hairdresser',\n",
       " 'incentives',\n",
       " 'ontario',\n",
       " 'crazies',\n",
       " 'crazier',\n",
       " 'grandma',\n",
       " 'enlighten',\n",
       " 'byte',\n",
       " 'bootloader',\n",
       " 'brasil',\n",
       " 'wrong',\n",
       " 'benching',\n",
       " 'initiate',\n",
       " 'sidequests',\n",
       " 'gordon',\n",
       " 'thurmond',\n",
       " 'neglect',\n",
       " 'hemorrhoids',\n",
       " 'emotion',\n",
       " 'assholish',\n",
       " 'oni',\n",
       " 'saving',\n",
       " 'ono',\n",
       " 'symmetry',\n",
       " 'holsters',\n",
       " 'parkinson',\n",
       " 'one',\n",
       " 'mignon',\n",
       " 'spokes',\n",
       " 'ons',\n",
       " 'symmetra',\n",
       " 'exaggerations',\n",
       " 'looong',\n",
       " 'stifles',\n",
       " 'stifled',\n",
       " 'constants',\n",
       " 'oversimplified',\n",
       " 'lingering',\n",
       " 'cutty',\n",
       " 'ableton',\n",
       " 'surges',\n",
       " 'brewpub',\n",
       " 'snatch',\n",
       " 'devito',\n",
       " 'modmail',\n",
       " 'herds',\n",
       " 'absorbs',\n",
       " 'surged',\n",
       " 'nougat',\n",
       " 'crossroads',\n",
       " 'rehab',\n",
       " 'disrespects',\n",
       " 'wandering',\n",
       " 'ascendancy',\n",
       " 'shakeup',\n",
       " 'illness',\n",
       " 'turned',\n",
       " 'locations',\n",
       " 'jewels',\n",
       " 'deadlifts',\n",
       " 'uninterrupted',\n",
       " 'turner',\n",
       " 'borough',\n",
       " 'leeching',\n",
       " 'leaderboards',\n",
       " 'zod',\n",
       " 'zoe',\n",
       " 'fashionable',\n",
       " 'coliseum',\n",
       " 'zoo',\n",
       " 'intends',\n",
       " 'orally',\n",
       " 'evened',\n",
       " 'concern',\n",
       " 'lick',\n",
       " 'pimple',\n",
       " 'imperatives',\n",
       " 'printer',\n",
       " 'offload',\n",
       " 'opposite',\n",
       " 'buffer',\n",
       " 'squalor',\n",
       " 'spewing',\n",
       " 'buffet',\n",
       " 'printed',\n",
       " 'knowingly',\n",
       " 'buffed',\n",
       " 'touchy',\n",
       " 'stigmas',\n",
       " 'phil',\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(embedding_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased Evaluation Results\n",
      "Biased MAC: 0.4497686945496877\n"
     ]
    }
   ],
   "source": [
    "print(\"Biased Evaluation Results\")\n",
    "biasedMAC, biasedDistribution = multiclass_evaluation(embedding_dict, evalTargets, evalAttrs)\n",
    "print(\"Biased MAC:\", biasedMAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARD Debiased Evaluation Results\n",
      "HARD MAC: 0.45832520102695906\n",
      "HARD Debiased Cosine difference t-test 0.00045344315625030706\n"
     ]
    }
   ],
   "source": [
    "print(\"HARD Debiased Evaluation Results\")\n",
    "debiasedMAC, debiasedDistribution = multiclass_evaluation(new_hard_word_vectors, evalTargets, evalAttrs)\n",
    "print(\"HARD MAC:\", debiasedMAC)\n",
    "\n",
    "statistics, pvalue = ttest_rel(biasedDistribution, debiasedDistribution)\n",
    "print(\"HARD Debiased Cosine difference t-test\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/\" + CFG.outprefix + \"_statistics-hard.csv\", \"w\")\n",
    "f.write(\"Biased MAC,Debiased MAC,P-Value\\n\")\n",
    "f.write(str(biasedMAC) + \",\" +  str(debiasedMAC) + \",\" + str(pvalue) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFT Debiased Evaluation Results\n",
      "soft MAC: 0.9861316682594786\n",
      "soft Debiased Cosine difference t-test 4.9834683545357994e-39\n"
     ]
    }
   ],
   "source": [
    "print(\"SOFT Debiased Evaluation Results\")\n",
    "debiasedMAC, debiasedDistribution = multiclass_evaluation(new_soft_word_vectors, evalTargets, evalAttrs)\n",
    "print(\"soft MAC:\", debiasedMAC)\n",
    "\n",
    "statistics, pvalue = ttest_rel(biasedDistribution, debiasedDistribution)\n",
    "print(\"soft Debiased Cosine difference t-test\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/\" + CFG.outprefix + \"_statistics-soft.csv\", \"w\")\n",
    "f.write(\"Biased MAC,Debiased MAC,P-Value\\n\")\n",
    "f.write(str(biasedMAC) + \",\" +  str(debiasedMAC) + \",\" + str(pvalue) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = [item[0] for item in evalTargets]\n",
    "female_words = [item[1] for item in evalTargets]\n",
    "attribute_female_professions = list(evalAttrs)[1]\n",
    "attribute_male_professions = list(evalAttrs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_biased_vectors = convert_legacy_to_keyvec(embedding_dict)\n",
    "# model = wefe.word_embedding_model.WordEmbeddingModel(gensim_biased_vectors, \"biased_vectors_model\")\n",
    "\n",
    "# query = wefe.query.Query([male_words, female_words], [attribute_male_professions, attribute_female_professions], ['male words', 'female words'], ['male professions', 'female professions'])\n",
    "\n",
    "# weat = wefe.metrics.WEAT()\n",
    "# result = weat.run_query(query, model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_hard_debiased_vectors = convert_legacy_to_keyvec(new_hard_word_vectors)\n",
    "# model = wefe.word_embedding_model.WordEmbeddingModel(gensim_hard_debiased_vectors, \"hard_debiased_vectors_model\")\n",
    "\n",
    "# query = wefe.query.Query([male_words, female_words], [attribute_male_professions, attribute_female_professions], ['male words', 'female words'], ['male professions', 'female professions'])\n",
    "\n",
    "# weat = wefe.metrics.WEAT()\n",
    "# result = weat.run_query(query, model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_soft_debiased_vectors = convert_legacy_to_keyvec(new_soft_word_vectors)\n",
    "# model = wefe.word_embedding_model.WordEmbeddingModel(gensim_soft_debiased_vectors, \"soft_debiased_vectors_model\")\n",
    "\n",
    "# query = wefe.query.Query([male_words, female_words], [attribute_male_professions, attribute_female_professions], ['male words', 'female words'], ['male professions', 'female professions'])\n",
    "\n",
    "# weat = wefe.metrics.WEAT()\n",
    "# result = weat.run_query(query, model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_soft_debiased_vectors = convert_legacy_to_keyvec(new_soft_word_vectors)\n",
    "# model = wefe.word_embedding_model.WordEmbeddingModel(gensim_soft_debiased_vectors, \"soft_debiased_vectors_model\")\n",
    "\n",
    "# query = wefe.query.Query([male_words, female_words], [attribute_male_professions, attribute_female_professions], ['male words', 'female words'], ['male professions', 'female professions'])\n",
    "\n",
    "# weat = wefe.metrics.MAC()\n",
    "# result = weat.run_query(query, model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CFG.stereoset_file) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in list(model.keys()):\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gender = [item for item in data['data']['intersentence'] if item['bias_type']=='gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'target', 'bias_type', 'context', 'sentences'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gender[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = []\n",
    "\n",
    "for item in data_gender:\n",
    "    data_dict = defaultdict()\n",
    "    data_dict['context'] = item['context']\n",
    "    for item_ in item['sentences']:\n",
    "        if item_['gold_label'] == 'stereotype':\n",
    "            data_dict['stereotype'] = item_['sentence']\n",
    "        elif item_['gold_label'] == 'anti-stereotype':\n",
    "            data_dict['anti-stereotype'] = item_['sentence']\n",
    "    data_.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stereotype Score: 0.5082644628099173\n"
     ]
    }
   ],
   "source": [
    "total_samples = 0\n",
    "stereotypical_samples = 0\n",
    "\n",
    "# Iterate over the data\n",
    "for item in data_:\n",
    "    context = item['context']\n",
    "    stereo = item['stereotype']\n",
    "    antistereo = item['anti-stereotype']\n",
    "\n",
    "    # Calculate sentence embeddings\n",
    "    context_vec = avg_feature_vector(context, new_soft_word_vectors, num_features=1024)\n",
    "    stereo_vec = avg_feature_vector(stereo, new_soft_word_vectors, num_features=1024)\n",
    "    antistereo_vec = avg_feature_vector(antistereo, new_soft_word_vectors, num_features=1024)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    simstereo = spatial.distance.cosine(context_vec, stereo_vec)\n",
    "    simantistereo = spatial.distance.cosine(context_vec, antistereo_vec)\n",
    "\n",
    "    if simstereo > simantistereo:\n",
    "        stereotypical_samples += 1\n",
    "    total_samples += 1\n",
    "\n",
    "# Calculate stereotype score\n",
    "stereotype_score = stereotypical_samples / total_samples\n",
    "print('Stereotype Score:', stereotype_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R106",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
